{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efb6ca7a-0008-4a65-8fe3-0050da6fb335",
   "metadata": {},
   "source": [
    "# 爬取原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "13ad77a6-3311-46c4-894b-3979b75fd7e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003011 |2003-03-30|04 05 11 12 30 32 |15 |12782494\n",
      "2003010 |2003-03-27|01 02 08 13 17 24 |13 |12402130\n",
      "2003009 |2003-03-23|05 09 18 20 22 30 |09 |12386072\n",
      "2003008 |2003-03-20|05 08 09 14 17 23 |08 |11696278\n",
      "2003007 |2003-03-16|01 09 19 21 23 26 |07 |12124030\n",
      "2003006 |2003-03-13|01 03 10 21 26 27 |06 |10919658\n",
      "2003005 |2003-03-09|04 06 15 17 30 31 |16 |10661438\n",
      "2003004 |2003-03-06|04 06 07 10 13 25 |03 |9517794\n",
      "2003003 |2003-03-02|01 07 10 23 28 32 |16 |8917960\n",
      "2003002 |2003-02-27|04 09 19 20 21 26 |12 |7398870\n",
      "2003001 |2003-02-23|10 11 12 13 26 28 |11 |10307806\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import random\n",
    "url = 'https://jc.zhcw.com/port/client_json.php'\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0\",\n",
    "    \"Referer\": \"https://www.zhcw.com/\"\n",
    "}\n",
    "cookies = {\n",
    "    \"Hm_lvt_692bd5f9c07d3ebd0063062fb0d7622f\": \"1701622572\",\n",
    "    \"_gid\": \"GA1.2.1218667687.1701622572\",\n",
    "    \"Hm_lvt_12e4883fd1649d006e3ae22a39f97330\": \"1701622573\",\n",
    "    \"PHPSESSID\": \"6k70gq2h44nksmou3n8374jq13\",\n",
    "    \"_ga_9FDP3NWFMS\": \"GS1.1.1701622572.1.1.1701623257.0.0.0\",\n",
    "    \"_ga\": \"GA1.2.1720843243.1701622572\",\n",
    "    \"Hm_lpvt_12e4883fd1649d006e3ae22a39f97330\": \"1701623257\",\n",
    "    \"Hm_lpvt_692bd5f9c07d3ebd0063062fb0d7622f\": \"1701623258\"\n",
    "}\n",
    "params = {\n",
    "        'callback':'jQuery112208474410773064831_1701622567918',\n",
    "        'transactionType':10001001,\n",
    "        'lotteryId':1,\n",
    "        'issueCount':0,\n",
    "        'startIssue':'',\n",
    "        'endIssue':'',\n",
    "        'startDate':'2003-02-01',\n",
    "        'endDate':'2003-04-01',\n",
    "        'type':2,\n",
    "        'pageNum':1,'pageSize':30,\n",
    "        \"tt\": random.random(),\n",
    "        \"_\": str(int(time.time() * 1000))\n",
    "    }\n",
    "# 发送HTTP GET请求\n",
    "response = requests.get(url,headers=headers,cookies=cookies,params=params)\n",
    "\n",
    "# 提取JSONP响应中的JSON数据\n",
    "json_data = response.text.split('(')[1].split(')')[0]\n",
    "# 解析JSON数据\n",
    "data = json.loads(json_data)\n",
    "# 提取双色球号码信息\n",
    "for entry in data['data']:\n",
    "    issue = entry['issue']\n",
    "    openTime = entry[\"openTime\"]\n",
    "    front_winning_num = entry['frontWinningNum']\n",
    "    back_winning_num = entry['backWinningNum']\n",
    "    saleMoney = entry[\"saleMoney\"]\n",
    "    print(f\"{issue} |{openTime}|{front_winning_num} |{back_winning_num} |{saleMoney}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729214b9-4ca8-4a8c-9d0c-17d848f0d20b",
   "metadata": {},
   "source": [
    "## 生成时间间隔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e8a2ab8-c7ca-474a-b4db-86ff1de575f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2003-02-01', '2003-04-02', '2003-06-01', '2003-07-31', '2003-09-29', '2003-11-28', '2004-01-27', '2004-03-27', '2004-05-26', '2004-07-25', '2004-09-23', '2004-11-22', '2005-01-21', '2005-03-22', '2005-05-21', '2005-07-20', '2005-09-18', '2005-11-17', '2006-01-16', '2006-03-17', '2006-05-16', '2006-07-15', '2006-09-13', '2006-11-12', '2007-01-11', '2007-03-12', '2007-05-11', '2007-07-10', '2007-09-08', '2007-11-07', '2008-01-06', '2008-03-06', '2008-05-05', '2008-07-04', '2008-09-02', '2008-11-01', '2008-12-31', '2009-03-01', '2009-04-30', '2009-06-29', '2009-08-28', '2009-10-27', '2009-12-26', '2010-02-24', '2010-04-25', '2010-06-24', '2010-08-23', '2010-10-22', '2010-12-21', '2011-02-19', '2011-04-20', '2011-06-19', '2011-08-18', '2011-10-17', '2011-12-16', '2012-02-14', '2012-04-14', '2012-06-13', '2012-08-12', '2012-10-11', '2012-12-10', '2013-02-08', '2013-04-09', '2013-06-08', '2013-08-07', '2013-10-06', '2013-12-05', '2014-02-03', '2014-04-04', '2014-06-03', '2014-08-02', '2014-10-01', '2014-11-30', '2015-01-29', '2015-03-30', '2015-05-29', '2015-07-28', '2015-09-26', '2015-11-25', '2016-01-24', '2016-03-24', '2016-05-23', '2016-07-22', '2016-09-20', '2016-11-19', '2017-01-18', '2017-03-19', '2017-05-18', '2017-07-17', '2017-09-15', '2017-11-14', '2018-01-13', '2018-03-14', '2018-05-13', '2018-07-12', '2018-09-10', '2018-11-09', '2019-01-08', '2019-03-09', '2019-05-08', '2019-07-07', '2019-09-05', '2019-11-04', '2020-01-03', '2020-03-03', '2020-05-02', '2020-07-01', '2020-08-30', '2020-10-29', '2020-12-28', '2021-02-26', '2021-04-27', '2021-06-26', '2021-08-25', '2021-10-24', '2021-12-23', '2022-02-21', '2022-04-22', '2022-06-21', '2022-08-20', '2022-10-19', '2022-12-18', '2023-02-16', '2023-04-17', '2023-06-16', '2023-08-15', '2023-10-14']\n",
      "['2003-04-01', '2003-05-31', '2003-07-30', '2003-09-28', '2003-11-27', '2004-01-26', '2004-03-26', '2004-05-25', '2004-07-24', '2004-09-22', '2004-11-21', '2005-01-20', '2005-03-21', '2005-05-20', '2005-07-19', '2005-09-17', '2005-11-16', '2006-01-15', '2006-03-16', '2006-05-15', '2006-07-14', '2006-09-12', '2006-11-11', '2007-01-10', '2007-03-11', '2007-05-10', '2007-07-09', '2007-09-07', '2007-11-06', '2008-01-05', '2008-03-05', '2008-05-04', '2008-07-03', '2008-09-01', '2008-10-31', '2008-12-30', '2009-02-28', '2009-04-29', '2009-06-28', '2009-08-27', '2009-10-26', '2009-12-25', '2010-02-23', '2010-04-24', '2010-06-23', '2010-08-22', '2010-10-21', '2010-12-20', '2011-02-18', '2011-04-19', '2011-06-18', '2011-08-17', '2011-10-16', '2011-12-15', '2012-02-13', '2012-04-13', '2012-06-12', '2012-08-11', '2012-10-10', '2012-12-09', '2013-02-07', '2013-04-08', '2013-06-07', '2013-08-06', '2013-10-05', '2013-12-04', '2014-02-02', '2014-04-03', '2014-06-02', '2014-08-01', '2014-09-30', '2014-11-29', '2015-01-28', '2015-03-29', '2015-05-28', '2015-07-27', '2015-09-25', '2015-11-24', '2016-01-23', '2016-03-23', '2016-05-22', '2016-07-21', '2016-09-19', '2016-11-18', '2017-01-17', '2017-03-18', '2017-05-17', '2017-07-16', '2017-09-14', '2017-11-13', '2018-01-12', '2018-03-13', '2018-05-12', '2018-07-11', '2018-09-09', '2018-11-08', '2019-01-07', '2019-03-08', '2019-05-07', '2019-07-06', '2019-09-04', '2019-11-03', '2020-01-02', '2020-03-02', '2020-05-01', '2020-06-30', '2020-08-29', '2020-10-28', '2020-12-27', '2021-02-25', '2021-04-26', '2021-06-25', '2021-08-24', '2021-10-23', '2021-12-22', '2022-02-20', '2022-04-21', '2022-06-20', '2022-08-19', '2022-10-18', '2022-12-17', '2023-02-15', '2023-04-16', '2023-06-15', '2023-08-14', '2023-10-13', '2023-12-04']\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def add_two_months(date_str):\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "    current_date = datetime.datetime.strptime(date_str, date_format)\n",
    "    now = datetime.datetime.now()\n",
    "    datestart = []\n",
    "    dateend = []\n",
    "    while current_date < now:\n",
    "        datestart.append(current_date.strftime(date_format))\n",
    "        current_date_end = current_date + datetime.timedelta(days=59)\n",
    "        if  current_date_end < now:\n",
    "            dateend.append(current_date_end.strftime(date_format))\n",
    "        else:\n",
    "            dateend.append(now.strftime(date_format))\n",
    "        current_date = current_date + datetime.timedelta(days=60)\n",
    "    return datestart,dateend\n",
    "start_date = \"2003-02-01\"\n",
    "datestart,dateend = add_two_months(start_date)\n",
    "print(datestart)\n",
    "print(dateend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3692ccb7-fff0-467a-a337-9613a618989d",
   "metadata": {},
   "source": [
    "## 爬取数据并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0239fb36-94f8-4213-b8ee-2e4d8769b332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = open('Bicolorballs.csv', 'w')  # 打开Bicolorballs.csv文件，写模式\n",
    "for s,e  in zip(datestart,dateend):\n",
    "    params = {\n",
    "        'callback':'jQuery112208474410773064831_1701622567918',\n",
    "        'transactionType':10001001,\n",
    "        'lotteryId':1,\n",
    "        'issueCount':0,\n",
    "        'startIssue':'',\n",
    "        'endIssue':'',\n",
    "        'startDate':s,\n",
    "        'endDate':e,\n",
    "        'type':2,\n",
    "        'pageNum':1,'pageSize':30,\n",
    "        \"tt\": random.random(),\n",
    "        \"_\": str(int(time.time() * 1000))\n",
    "    }\n",
    "    # 发送HTTP GET请求\n",
    "    response = requests.get(url,headers=headers,cookies=cookies,params=params)\n",
    "\n",
    "    # 提取JSONP响应中的JSON数据\n",
    "    json_data = response.text.split('(')[1].split(')')[0]\n",
    "    # 解析JSON数据\n",
    "    data = json.loads(json_data)\n",
    "    # 提取双色球号码信息\n",
    "    for entry in data['data']:\n",
    "        issue = entry['issue']\n",
    "        openTime = entry[\"openTime\"]\n",
    "        front_winning_num = entry['frontWinningNum']\n",
    "        back_winning_num = entry['backWinningNum']\n",
    "        saleMoney = entry[\"saleMoney\"]\n",
    "        prizePoolMoney = entry[\"prizePoolMoney\"]\n",
    "        data = f\"{issue},{openTime},{front_winning_num},{back_winning_num},{saleMoney},{prizePoolMoney}\\r\\n\"\n",
    "        file.write(data)\n",
    "file.close()  # 关闭文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce2e2c0-6bf8-4e58-966a-3dfa6f36a6e0",
   "metadata": {},
   "source": [
    "## 可视化销售数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db70d908-4f6e-43c0-b5f8-ed7b2d0899ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 打开CSV文件\n",
    "with open('Bicolorballs.csv', 'r') as file:\n",
    "    # 创建CSV读取器\n",
    "    reader = csv.reader(file)\n",
    "    # 初始化列表来保存最后一列和倒数第二列的数据\n",
    "    dates = []\n",
    "    last_column = []\n",
    "    second_last_column = []\n",
    "    # 逐行读取CSV文件\n",
    "    for row in reader:\n",
    "        # 提取最后一列和倒数第二列的数据\n",
    "        date = datetime.datetime.strptime(row[1], '%Y/%m/%d')\n",
    "        last_value = float(row[-1])\n",
    "        second_last_value = float(row[-2])\n",
    "        # 添加到列表中\n",
    "        dates.append(date)\n",
    "        last_column.append(last_value)\n",
    "        second_last_column.append(second_last_value)\n",
    "\n",
    "# 绘制曲线\n",
    "plt.figure(figsize=(20, 8)) \n",
    "plt.plot(dates, last_column, label='prizePoolMoney')\n",
    "plt.plot(dates, second_last_column, label='saleMoney')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('2003-2023 Bicolorballs saleMoney and prizePoolMoney')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62beda2-2929-4e62-8738-db41b020b200",
   "metadata": {},
   "source": [
    "## 可视化中奖球频次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "98e9ccea-0138-47e5-90a1-a215f84b190c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 打开CSV文件\n",
    "with open('Bicolorballs.csv', 'r') as file:\n",
    "    # 创建CSV读取器\n",
    "    reader = csv.reader(file)\n",
    "    # 初始化列表来保存最后一列和倒数第二列的数据\n",
    "    dates = []\n",
    "    redL = [0 for i in range(34)]\n",
    "    blueL = [0 for i in range(17)]\n",
    "    # 逐行读取CSV文件\n",
    "    for row in reader:\n",
    "        # 提取最后一列和倒数第二列的数据\n",
    "        red = row[2].split(' ')\n",
    "        for r in red:\n",
    "            redL[int(r)]=redL[int(r)]+1\n",
    "        blue = row[3]\n",
    "        blueL[int(blue)]=blueL[int(blue)]+1\n",
    "# 绘制曲线\n",
    "\n",
    "# 统计数据\n",
    "red_counts = redL[1:]  # 去除索引为0的元素\n",
    "blue_counts = blueL[1:]  # 去除索引为0的元素\n",
    "# 设置红色和蓝色球的标签\n",
    "red_labels = [str(i) for i in range(1, 34)]\n",
    "blue_labels = [str(i) for i in range(1, 17)]\n",
    "\n",
    "# 绘制直方图\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
    "ax1.bar(red_labels, red_counts, color='red', label='Red Balls')\n",
    "ax2.bar(blue_labels, blue_counts, color='blue', label='Blue Balls')\n",
    "\n",
    "# 添加标题和标签\n",
    "ax2.bar(blue_labels, blue_counts, color='blue')\n",
    "ax2.set_xlabel('Blue Balls')\n",
    "ax2.set_ylabel('Frequency')\n",
    "# 绘制红色球的直方图\n",
    "ax1.bar(red_labels, red_counts, color='red')\n",
    "ax1.set_xlabel('Red Balls')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9dd91011-f550-45df-8c29-f5c8f2a419de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[501, 519, 526, 527, 528, 534, 539, 539, 539, 541, 545, 545, 553, 556, 556, 556, 558, 567, 567, 567, 569, 571, 578, 579, 581, 582, 585, 585, 586, 594, 597, 614, 614]\n",
      "频次 614  编号 14\n",
      "频次 614  编号 14\n",
      "频次 597  编号 1\n",
      "频次 594  编号 22\n",
      "频次 586  编号 6\n",
      "频次 585  编号 17\n",
      "频次 213  编号 1\n"
     ]
    }
   ],
   "source": [
    "# 对数组进行排序\n",
    "sorted_array = sorted(red_counts)\n",
    "print(sorted_array)\n",
    "for i in range(6):\n",
    "    # 找到最大值的索引\n",
    "    index = red_counts.index(sorted_array[-i-1])\n",
    "    # 打印最大值和索引\n",
    "    print(\"频次\", sorted_array[-i-1],end = '  ')\n",
    "    print(\"编号\", index+1)\n",
    "# 对数组进行排序\n",
    "sorted_array = sorted(blue_counts)\n",
    "index = blue_counts.index(sorted_array[-1])\n",
    "print(\"频次\", sorted_array[-1],end = '  ')\n",
    "print(\"编号\", index+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3cdcc-320e-45c4-85c0-e6b667ae542a",
   "metadata": {},
   "source": [
    "# 构造数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb81f6-254b-481d-9b73-66e1e29c9d90",
   "metadata": {},
   "source": [
    "## 数据编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "883ae2d9-58a8-425c-84b0-ceb640e1c73b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集长度为： 3083\n",
      "红球编码后为： [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "蓝球编码后为： [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "with open('Bicolorballs.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    red_balls = []\n",
    "    blue_balls = []\n",
    "    for row in reader:\n",
    "        red = np.array([int(i)-1 for i in row[2].split(' ')])\n",
    "        # 编码\n",
    "        red = tf.one_hot(red, depth=33)\n",
    "        red = tf.reduce_sum(red, axis=0)\n",
    "        red_balls.append(red)\n",
    "        blue = np.array([int(row[3])-1])\n",
    "        blue = tf.one_hot(blue, depth=16)\n",
    "        blue = tf.reduce_sum(blue, axis=0)\n",
    "        blue_balls.append(blue)\n",
    "print(\"数据集长度为：\",len(red_balls))\n",
    "print(\"红球编码后为：\",red_balls[0].numpy())\n",
    "print(\"蓝球编码后为：\",blue_balls[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4cab9-85ad-4111-9697-dcbfb5c64b56",
   "metadata": {},
   "source": [
    "## 数据集构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e1974155-5992-4b8d-8413-6bf9e6ef70ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = tf.stack(red_balls)\n",
    "# 定义超参数\n",
    "n_steps = 150  # 时间步数，即每个样本包含的历史时间步数\n",
    "T = len(data)\n",
    "features = []\n",
    "for i in range(T-n_steps):\n",
    "    features.append(data[i: n_steps + i,:])\n",
    "labels = data[n_steps:,:]\n",
    "features = tf.stack(features) \n",
    "\n",
    "# 将数据集划分为训练集和测试集\n",
    "train_size = int(len(data) * 0.8)\n",
    "train_X, train_y = features[:train_size], labels[:train_size]\n",
    "test_X, test_y = features[train_size:],labels[train_size:]\n",
    "\n",
    "# 创建tf.data.Dataset对象\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_X, train_y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_X, test_y))\n",
    "\n",
    "# 可选：对数据集进行一些预处理操作，例如乱序、批量化和缓存等\n",
    "train_dataset = train_dataset.shuffle(100).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bcc4d836-0e24-4176-ab22-30968562499f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 150, 33)\n"
     ]
    }
   ],
   "source": [
    "data,label = iter(train_dataset.take(1)).next()\n",
    "i = 3\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b29fc65-992d-424e-beef-f816469afcec",
   "metadata": {},
   "source": [
    "# 模型构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e02ee1b4-2050-4d67-892c-22cee6635451",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 150, 64)           25088     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 33)                2145      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,257\n",
      "Trainable params: 60,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "# 定义输入和输出形状\n",
    "input_shape = (150, 33)\n",
    "output_shape = (1, 33)\n",
    "# 创建双层LSTM网络\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=input_shape))  # 第一层LSTM\n",
    "model.add(LSTM(64))  # 第二层LSTM\n",
    "model.add(Dense(output_shape[-1], activation='sigmoid'))  # 输出层\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# 打印模型结构\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01602a-2dfb-49e9-b5eb-a156dc779c4f",
   "metadata": {},
   "source": [
    "## 红球模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5767af32-81c4-408e-988b-7a04a4aef4d3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20231204-174742LSTM2_lr100_batchsize\n",
      "Epoch 1/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 23.0080 - accuracy: 0.0088\n",
      "Epoch 1: val_accuracy improved from -inf to 0.00857, saving model to ./model\\20231204-174742LSTM2_lr100_batchsize.h5\n",
      "78/78 [==============================] - 4s 26ms/step - loss: 23.0241 - accuracy: 0.0089 - val_loss: 23.5957 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 2/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 23.0925 - accuracy: 0.0103\n",
      "Epoch 2: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 23.0918 - accuracy: 0.0105 - val_loss: 22.7870 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 3/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 22.5857 - accuracy: 0.0095\n",
      "Epoch 3: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 22.5785 - accuracy: 0.0097 - val_loss: 22.4689 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 4/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 22.4268 - accuracy: 0.0101\n",
      "Epoch 4: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 22.4268 - accuracy: 0.0101 - val_loss: 22.3341 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 5/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 22.2263 - accuracy: 0.0097\n",
      "Epoch 5: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 22.2266 - accuracy: 0.0097 - val_loss: 22.1985 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 6/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 22.1056 - accuracy: 0.0097\n",
      "Epoch 6: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 22.1055 - accuracy: 0.0097 - val_loss: 22.0885 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 7/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 22.0095 - accuracy: 0.0100\n",
      "Epoch 7: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 22.0095 - accuracy: 0.0097 - val_loss: 21.9877 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 8/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.8730 - accuracy: 0.0099\n",
      "Epoch 8: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.8763 - accuracy: 0.0097 - val_loss: 21.8957 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 9/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.8166 - accuracy: 0.0081\n",
      "Epoch 9: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.8166 - accuracy: 0.0081 - val_loss: 21.8545 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 10/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.8254 - accuracy: 0.0099\n",
      "Epoch 10: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.8278 - accuracy: 0.0097 - val_loss: 21.8089 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 11/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.8169 - accuracy: 0.0089\n",
      "Epoch 11: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.8169 - accuracy: 0.0089 - val_loss: 21.7982 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 12/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.7404 - accuracy: 0.0095\n",
      "Epoch 12: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.7415 - accuracy: 0.0093 - val_loss: 21.7433 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 13/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.7424 - accuracy: 0.0095\n",
      "Epoch 13: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.7364 - accuracy: 0.0097 - val_loss: 21.7439 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 14/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.6845 - accuracy: 0.0097\n",
      "Epoch 14: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.6847 - accuracy: 0.0097 - val_loss: 21.6714 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 15/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.6201 - accuracy: 0.0099\n",
      "Epoch 15: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.6227 - accuracy: 0.0097 - val_loss: 21.6384 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 16/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.6068 - accuracy: 0.0095\n",
      "Epoch 16: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.6084 - accuracy: 0.0097 - val_loss: 21.6340 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 17/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.6296 - accuracy: 0.0096\n",
      "Epoch 17: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.6269 - accuracy: 0.0097 - val_loss: 21.6201 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 18/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.6111 - accuracy: 0.0096\n",
      "Epoch 18: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.6120 - accuracy: 0.0097 - val_loss: 21.6048 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 19/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.5974 - accuracy: 0.0097\n",
      "Epoch 19: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.5972 - accuracy: 0.0097 - val_loss: 21.6040 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 20/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.5505 - accuracy: 0.0095\n",
      "Epoch 20: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.5505 - accuracy: 0.0097 - val_loss: 21.5382 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 21/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.5167 - accuracy: 0.0096\n",
      "Epoch 21: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.5147 - accuracy: 0.0097 - val_loss: 21.5443 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 22/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.5259 - accuracy: 0.0095\n",
      "Epoch 22: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.5274 - accuracy: 0.0093 - val_loss: 21.5119 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 23/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.4506 - accuracy: 0.0097\n",
      "Epoch 23: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.4505 - accuracy: 0.0097 - val_loss: 21.4576 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 24/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.4324 - accuracy: 0.0097\n",
      "Epoch 24: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.4324 - accuracy: 0.0097 - val_loss: 21.4388 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 25/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.4283 - accuracy: 0.0099\n",
      "Epoch 25: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.4258 - accuracy: 0.0097 - val_loss: 21.4456 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 26/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.4358 - accuracy: 0.0097\n",
      "Epoch 26: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.4361 - accuracy: 0.0097 - val_loss: 21.4314 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 27/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.3966 - accuracy: 0.0097\n",
      "Epoch 27: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.3966 - accuracy: 0.0097 - val_loss: 21.3929 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 28/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.3327 - accuracy: 0.0096\n",
      "Epoch 28: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.3355 - accuracy: 0.0097 - val_loss: 21.3735 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 29/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.3497 - accuracy: 0.0100\n",
      "Epoch 29: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.3483 - accuracy: 0.0097 - val_loss: 21.3769 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 30/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.4017 - accuracy: 0.0096\n",
      "Epoch 30: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.3981 - accuracy: 0.0097 - val_loss: 21.3897 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 31/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.3624 - accuracy: 0.0097\n",
      "Epoch 31: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 24ms/step - loss: 21.3632 - accuracy: 0.0097 - val_loss: 21.3536 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 32/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.3135 - accuracy: 0.0097\n",
      "Epoch 32: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 21.3139 - accuracy: 0.0097 - val_loss: 21.3188 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 33/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.3014 - accuracy: 0.0097\n",
      "Epoch 33: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.3014 - accuracy: 0.0097 - val_loss: 21.3236 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 34/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.3518 - accuracy: 0.0097\n",
      "Epoch 34: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.3520 - accuracy: 0.0097 - val_loss: 21.3317 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 35/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.3191 - accuracy: 0.0096\n",
      "Epoch 35: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.3183 - accuracy: 0.0093 - val_loss: 21.3111 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 36/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.2784 - accuracy: 0.0097\n",
      "Epoch 36: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.2783 - accuracy: 0.0097 - val_loss: 21.3096 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 37/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.2941 - accuracy: 0.0097\n",
      "Epoch 37: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.2943 - accuracy: 0.0097 - val_loss: 21.2983 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 38/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.2794 - accuracy: 0.0100\n",
      "Epoch 38: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.2810 - accuracy: 0.0097 - val_loss: 21.3118 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 39/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.2811 - accuracy: 0.0097\n",
      "Epoch 39: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.2811 - accuracy: 0.0097 - val_loss: 21.2995 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 40/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.2456 - accuracy: 0.0097\n",
      "Epoch 40: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.2456 - accuracy: 0.0097 - val_loss: 21.2773 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 41/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.2702 - accuracy: 0.0099\n",
      "Epoch 41: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.2700 - accuracy: 0.0097 - val_loss: 21.2980 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 42/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.3178 - accuracy: 0.0097\n",
      "Epoch 42: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.3178 - accuracy: 0.0097 - val_loss: 21.3080 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 43/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.2742 - accuracy: 0.0097\n",
      "Epoch 43: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.2742 - accuracy: 0.0097 - val_loss: 21.3018 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 44/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.2570 - accuracy: 0.0097\n",
      "Epoch 44: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.2568 - accuracy: 0.0097 - val_loss: 21.2741 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 45/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.2589 - accuracy: 0.0099\n",
      "Epoch 45: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.2583 - accuracy: 0.0097 - val_loss: 21.2777 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 46/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.2512 - accuracy: 0.0099\n",
      "Epoch 46: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.2491 - accuracy: 0.0097 - val_loss: 21.2670 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 47/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.2562 - accuracy: 0.0092\n",
      "Epoch 47: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.2559 - accuracy: 0.0097 - val_loss: 21.2784 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 48/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.2647 - accuracy: 0.0100\n",
      "Epoch 48: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.2649 - accuracy: 0.0097 - val_loss: 21.2708 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 49/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.2491 - accuracy: 0.0097\n",
      "Epoch 49: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.2491 - accuracy: 0.0097 - val_loss: 21.2478 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 50/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.2069 - accuracy: 0.0097\n",
      "Epoch 50: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.2067 - accuracy: 0.0097 - val_loss: 21.2197 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 51/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.2367 - accuracy: 0.0100\n",
      "Epoch 51: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.2347 - accuracy: 0.0097 - val_loss: 21.2319 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 52/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.1993 - accuracy: 0.0097\n",
      "Epoch 52: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1993 - accuracy: 0.0097 - val_loss: 21.2253 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 53/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.2210 - accuracy: 0.0096\n",
      "Epoch 53: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.2215 - accuracy: 0.0097 - val_loss: 21.2376 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 54/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.1996 - accuracy: 0.0097\n",
      "Epoch 54: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1991 - accuracy: 0.0097 - val_loss: 21.2145 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 55/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.2351 - accuracy: 0.0097\n",
      "Epoch 55: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.2351 - accuracy: 0.0097 - val_loss: 21.2254 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 56/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.2482 - accuracy: 0.0099\n",
      "Epoch 56: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.2473 - accuracy: 0.0097 - val_loss: 21.2276 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 57/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.1985 - accuracy: 0.0097\n",
      "Epoch 57: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1985 - accuracy: 0.0097 - val_loss: 21.1906 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 58/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.1921 - accuracy: 0.0097\n",
      "Epoch 58: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1921 - accuracy: 0.0097 - val_loss: 21.2053 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 59/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.1797 - accuracy: 0.0097\n",
      "Epoch 59: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1797 - accuracy: 0.0097 - val_loss: 21.1970 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 60/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.1941 - accuracy: 0.0097\n",
      "Epoch 60: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1937 - accuracy: 0.0097 - val_loss: 21.2081 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 61/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.2023 - accuracy: 0.0095\n",
      "Epoch 61: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.2034 - accuracy: 0.0093 - val_loss: 21.2173 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 62/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.2065 - accuracy: 0.0099\n",
      "Epoch 62: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.2069 - accuracy: 0.0097 - val_loss: 21.2119 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 63/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.1712 - accuracy: 0.0100\n",
      "Epoch 63: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1698 - accuracy: 0.0097 - val_loss: 21.1720 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 64/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.1736 - accuracy: 0.0099\n",
      "Epoch 64: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.1721 - accuracy: 0.0097 - val_loss: 21.1836 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 65/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.1889 - accuracy: 0.0097\n",
      "Epoch 65: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1889 - accuracy: 0.0097 - val_loss: 21.1879 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 66/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.1796 - accuracy: 0.0097\n",
      "Epoch 66: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1799 - accuracy: 0.0097 - val_loss: 21.1787 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 67/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.1509 - accuracy: 0.0096\n",
      "Epoch 67: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1526 - accuracy: 0.0097 - val_loss: 21.1642 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 68/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.1400 - accuracy: 0.0097\n",
      "Epoch 68: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.1400 - accuracy: 0.0097 - val_loss: 21.1571 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 69/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.1698 - accuracy: 0.0099\n",
      "Epoch 69: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.1698 - accuracy: 0.0097 - val_loss: 21.1724 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 70/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.1502 - accuracy: 0.0097\n",
      "Epoch 70: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1502 - accuracy: 0.0097 - val_loss: 21.1740 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 71/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.1712 - accuracy: 0.0099\n",
      "Epoch 71: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.1716 - accuracy: 0.0097 - val_loss: 21.1794 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 72/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.1610 - accuracy: 0.0097\n",
      "Epoch 72: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1610 - accuracy: 0.0097 - val_loss: 21.1533 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 73/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.1290 - accuracy: 0.0096\n",
      "Epoch 73: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.1265 - accuracy: 0.0097 - val_loss: 21.1422 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 74/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.1397 - accuracy: 0.0100\n",
      "Epoch 74: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1386 - accuracy: 0.0097 - val_loss: 21.1557 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 75/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.1311 - accuracy: 0.0097\n",
      "Epoch 75: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1311 - accuracy: 0.0097 - val_loss: 21.1536 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 76/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.1649 - accuracy: 0.0101\n",
      "Epoch 76: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.1648 - accuracy: 0.0101 - val_loss: 21.1775 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 77/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.1571 - accuracy: 0.0097\n",
      "Epoch 77: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1571 - accuracy: 0.0097 - val_loss: 21.1664 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 78/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.1462 - accuracy: 0.0095\n",
      "Epoch 78: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1476 - accuracy: 0.0097 - val_loss: 21.1610 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 79/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.1468 - accuracy: 0.0097\n",
      "Epoch 79: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1464 - accuracy: 0.0097 - val_loss: 21.1489 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 80/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.1537 - accuracy: 0.0097\n",
      "Epoch 80: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1539 - accuracy: 0.0097 - val_loss: 21.1611 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 81/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.1249 - accuracy: 0.0097\n",
      "Epoch 81: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1249 - accuracy: 0.0097 - val_loss: 21.1398 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 82/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.1263 - accuracy: 0.0097\n",
      "Epoch 82: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1263 - accuracy: 0.0097 - val_loss: 21.1486 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 83/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.1404 - accuracy: 0.0100\n",
      "Epoch 83: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1406 - accuracy: 0.0097 - val_loss: 21.1519 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 84/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.1197 - accuracy: 0.0097\n",
      "Epoch 84: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1197 - accuracy: 0.0097 - val_loss: 21.1389 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 85/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.1257 - accuracy: 0.0095\n",
      "Epoch 85: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1244 - accuracy: 0.0097 - val_loss: 21.1384 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 86/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.1195 - accuracy: 0.0097\n",
      "Epoch 86: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1195 - accuracy: 0.0097 - val_loss: 21.1339 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 87/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.1141 - accuracy: 0.0097\n",
      "Epoch 87: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1140 - accuracy: 0.0097 - val_loss: 21.1352 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 88/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.1271 - accuracy: 0.0099\n",
      "Epoch 88: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1284 - accuracy: 0.0097 - val_loss: 21.1315 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 89/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.1026 - accuracy: 0.0097\n",
      "Epoch 89: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.1024 - accuracy: 0.0097 - val_loss: 21.1293 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 90/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.1246 - accuracy: 0.0100\n",
      "Epoch 90: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.1255 - accuracy: 0.0097 - val_loss: 21.1396 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 91/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.1289 - accuracy: 0.0097\n",
      "Epoch 91: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1289 - accuracy: 0.0097 - val_loss: 21.1339 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 92/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.1093 - accuracy: 0.0097\n",
      "Epoch 92: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 24ms/step - loss: 21.1092 - accuracy: 0.0097 - val_loss: 21.1310 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 93/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.1146 - accuracy: 0.0097\n",
      "Epoch 93: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1146 - accuracy: 0.0097 - val_loss: 21.1341 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 94/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.1093 - accuracy: 0.0097\n",
      "Epoch 94: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.1091 - accuracy: 0.0097 - val_loss: 21.1338 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 95/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.1263 - accuracy: 0.0097\n",
      "Epoch 95: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1262 - accuracy: 0.0097 - val_loss: 21.1421 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 96/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.1265 - accuracy: 0.0099\n",
      "Epoch 96: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.1269 - accuracy: 0.0097 - val_loss: 21.1433 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 97/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.1272 - accuracy: 0.0099\n",
      "Epoch 97: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.1257 - accuracy: 0.0097 - val_loss: 21.1390 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 98/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.1314 - accuracy: 0.0095\n",
      "Epoch 98: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.1325 - accuracy: 0.0097 - val_loss: 21.1452 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 99/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.1366 - accuracy: 0.0097\n",
      "Epoch 99: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.1365 - accuracy: 0.0097 - val_loss: 21.1514 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 100/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.1243 - accuracy: 0.0100\n",
      "Epoch 100: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.1220 - accuracy: 0.0097 - val_loss: 21.1355 - val_accuracy: 0.0086 - lr: 0.0010\n",
      "Epoch 101/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0978 - accuracy: 0.0097\n",
      "Epoch 101: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0978 - accuracy: 0.0097 - val_loss: 21.1263 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 102/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0979 - accuracy: 0.0099\n",
      "Epoch 102: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.0982 - accuracy: 0.0097 - val_loss: 21.1203 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 103/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0980 - accuracy: 0.0095\n",
      "Epoch 103: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0985 - accuracy: 0.0097 - val_loss: 21.1197 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 104/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0788 - accuracy: 0.0097\n",
      "Epoch 104: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0789 - accuracy: 0.0097 - val_loss: 21.1060 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 105/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0824 - accuracy: 0.0099\n",
      "Epoch 105: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0805 - accuracy: 0.0097 - val_loss: 21.1040 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 106/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0880 - accuracy: 0.0099\n",
      "Epoch 106: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0877 - accuracy: 0.0097 - val_loss: 21.1133 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 107/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0876 - accuracy: 0.0097\n",
      "Epoch 107: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0876 - accuracy: 0.0097 - val_loss: 21.1189 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 108/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0934 - accuracy: 0.0095\n",
      "Epoch 108: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0928 - accuracy: 0.0097 - val_loss: 21.1237 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 109/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0874 - accuracy: 0.0097\n",
      "Epoch 109: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0878 - accuracy: 0.0097 - val_loss: 21.1129 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 110/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0749 - accuracy: 0.0097\n",
      "Epoch 110: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0746 - accuracy: 0.0097 - val_loss: 21.1016 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 111/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0834 - accuracy: 0.0096\n",
      "Epoch 111: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0842 - accuracy: 0.0097 - val_loss: 21.1171 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 112/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0917 - accuracy: 0.0096\n",
      "Epoch 112: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0926 - accuracy: 0.0097 - val_loss: 21.1194 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 113/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0911 - accuracy: 0.0097\n",
      "Epoch 113: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0909 - accuracy: 0.0097 - val_loss: 21.1217 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 114/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0855 - accuracy: 0.0100\n",
      "Epoch 114: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0853 - accuracy: 0.0097 - val_loss: 21.1149 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 115/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0703 - accuracy: 0.0099\n",
      "Epoch 115: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.0702 - accuracy: 0.0097 - val_loss: 21.0979 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 116/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0700 - accuracy: 0.0099\n",
      "Epoch 116: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0705 - accuracy: 0.0097 - val_loss: 21.1032 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 117/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0668 - accuracy: 0.0100\n",
      "Epoch 117: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.0675 - accuracy: 0.0097 - val_loss: 21.1025 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 118/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0710 - accuracy: 0.0099\n",
      "Epoch 118: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.0716 - accuracy: 0.0097 - val_loss: 21.0981 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 119/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0722 - accuracy: 0.0097\n",
      "Epoch 119: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0722 - accuracy: 0.0097 - val_loss: 21.1016 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 120/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0716 - accuracy: 0.0099\n",
      "Epoch 120: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0700 - accuracy: 0.0097 - val_loss: 21.0909 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 121/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0740 - accuracy: 0.0097\n",
      "Epoch 121: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0740 - accuracy: 0.0097 - val_loss: 21.0901 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 122/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0639 - accuracy: 0.0099\n",
      "Epoch 122: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0655 - accuracy: 0.0097 - val_loss: 21.0887 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 123/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0571 - accuracy: 0.0097\n",
      "Epoch 123: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0571 - accuracy: 0.0097 - val_loss: 21.0786 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 124/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0537 - accuracy: 0.0099\n",
      "Epoch 124: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0537 - accuracy: 0.0097 - val_loss: 21.0870 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 125/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0647 - accuracy: 0.0096\n",
      "Epoch 125: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0651 - accuracy: 0.0097 - val_loss: 21.0981 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 126/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0680 - accuracy: 0.0099\n",
      "Epoch 126: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 21.0655 - accuracy: 0.0097 - val_loss: 21.0911 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 127/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0650 - accuracy: 0.0097\n",
      "Epoch 127: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0645 - accuracy: 0.0097 - val_loss: 21.0941 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 128/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0753 - accuracy: 0.0096\n",
      "Epoch 128: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.0753 - accuracy: 0.0097 - val_loss: 21.0961 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 129/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0687 - accuracy: 0.0095\n",
      "Epoch 129: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0681 - accuracy: 0.0097 - val_loss: 21.0854 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 130/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0587 - accuracy: 0.0097\n",
      "Epoch 130: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0587 - accuracy: 0.0097 - val_loss: 21.0839 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 131/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0528 - accuracy: 0.0097\n",
      "Epoch 131: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0528 - accuracy: 0.0097 - val_loss: 21.0823 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 132/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0647 - accuracy: 0.0794\n",
      "Epoch 132: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0659 - accuracy: 0.0783 - val_loss: 21.0909 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 133/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0641 - accuracy: 0.0049\n",
      "Epoch 133: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0641 - accuracy: 0.0049 - val_loss: 21.0866 - val_accuracy: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 134/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0534 - accuracy: 0.0093\n",
      "Epoch 134: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0532 - accuracy: 0.0093 - val_loss: 21.0732 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 135/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0568 - accuracy: 0.0099\n",
      "Epoch 135: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0577 - accuracy: 0.0097 - val_loss: 21.0754 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 136/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0582 - accuracy: 0.0095\n",
      "Epoch 136: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0588 - accuracy: 0.0097 - val_loss: 21.0744 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 137/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0602 - accuracy: 0.0123\n",
      "Epoch 137: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0614 - accuracy: 0.0122 - val_loss: 21.0790 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 138/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0572 - accuracy: 0.0394\n",
      "Epoch 138: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0575 - accuracy: 0.0393 - val_loss: 21.0673 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 139/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0471 - accuracy: 0.0095\n",
      "Epoch 139: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.0475 - accuracy: 0.0097 - val_loss: 21.0645 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 140/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0455 - accuracy: 0.0095\n",
      "Epoch 140: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0448 - accuracy: 0.0097 - val_loss: 21.0615 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 141/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0452 - accuracy: 0.0097\n",
      "Epoch 141: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0452 - accuracy: 0.0097 - val_loss: 21.0652 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 142/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0490 - accuracy: 0.0097\n",
      "Epoch 142: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.0490 - accuracy: 0.0097 - val_loss: 21.0658 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 143/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0503 - accuracy: 0.0100\n",
      "Epoch 143: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0520 - accuracy: 0.0097 - val_loss: 21.0706 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 144/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0502 - accuracy: 0.0097\n",
      "Epoch 144: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0501 - accuracy: 0.0097 - val_loss: 21.0672 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 145/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0529 - accuracy: 0.0099\n",
      "Epoch 145: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0530 - accuracy: 0.0097 - val_loss: 21.0691 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 146/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0415 - accuracy: 0.0099\n",
      "Epoch 146: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0426 - accuracy: 0.0097 - val_loss: 21.0588 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 147/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0414 - accuracy: 0.0097\n",
      "Epoch 147: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0418 - accuracy: 0.0097 - val_loss: 21.0564 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 148/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0366 - accuracy: 0.0096\n",
      "Epoch 148: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0370 - accuracy: 0.0097 - val_loss: 21.0474 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 149/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0495 - accuracy: 0.0097\n",
      "Epoch 149: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.0496 - accuracy: 0.0097 - val_loss: 21.0591 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 150/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0492 - accuracy: 0.0097\n",
      "Epoch 150: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0492 - accuracy: 0.0097 - val_loss: 21.0557 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 151/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0526 - accuracy: 0.0097\n",
      "Epoch 151: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0530 - accuracy: 0.0097 - val_loss: 21.0586 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 152/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0464 - accuracy: 0.0100\n",
      "Epoch 152: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 21.0457 - accuracy: 0.0097 - val_loss: 21.0554 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 153/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0489 - accuracy: 0.0099\n",
      "Epoch 153: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 21.0508 - accuracy: 0.0097 - val_loss: 21.0682 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 154/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0666 - accuracy: 0.0096\n",
      "Epoch 154: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0675 - accuracy: 0.0097 - val_loss: 21.0788 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 155/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0562 - accuracy: 0.0095\n",
      "Epoch 155: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0570 - accuracy: 0.0097 - val_loss: 21.0687 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 156/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0431 - accuracy: 0.0099\n",
      "Epoch 156: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.0450 - accuracy: 0.0097 - val_loss: 21.0641 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 157/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0406 - accuracy: 0.0096\n",
      "Epoch 157: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0420 - accuracy: 0.0097 - val_loss: 21.0631 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 158/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0469 - accuracy: 0.0097\n",
      "Epoch 158: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0473 - accuracy: 0.0097 - val_loss: 21.0596 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 159/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0427 - accuracy: 0.0099\n",
      "Epoch 159: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0433 - accuracy: 0.0097 - val_loss: 21.0536 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 160/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0373 - accuracy: 0.0095\n",
      "Epoch 160: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0384 - accuracy: 0.0097 - val_loss: 21.0547 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 161/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0392 - accuracy: 0.0096\n",
      "Epoch 161: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0383 - accuracy: 0.0097 - val_loss: 21.0602 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 162/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0492 - accuracy: 0.0097\n",
      "Epoch 162: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0492 - accuracy: 0.0097 - val_loss: 21.0708 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 163/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0507 - accuracy: 0.0097\n",
      "Epoch 163: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0507 - accuracy: 0.0097 - val_loss: 21.0690 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 164/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0595 - accuracy: 0.0100\n",
      "Epoch 164: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0591 - accuracy: 0.0097 - val_loss: 21.0761 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 165/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0593 - accuracy: 0.0095\n",
      "Epoch 165: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0591 - accuracy: 0.0097 - val_loss: 21.0764 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 166/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0579 - accuracy: 0.0097\n",
      "Epoch 166: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0579 - accuracy: 0.0097 - val_loss: 21.0741 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 167/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0413 - accuracy: 0.0095\n",
      "Epoch 167: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.0418 - accuracy: 0.0097 - val_loss: 21.0599 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 168/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0423 - accuracy: 0.0097\n",
      "Epoch 168: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.0423 - accuracy: 0.0097 - val_loss: 21.0629 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 169/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0529 - accuracy: 0.0095\n",
      "Epoch 169: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0521 - accuracy: 0.0097 - val_loss: 21.0726 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 170/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0575 - accuracy: 0.0519\n",
      "Epoch 170: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0578 - accuracy: 0.0519 - val_loss: 21.0717 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 171/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0504 - accuracy: 0.0099\n",
      "Epoch 171: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0505 - accuracy: 0.0097 - val_loss: 21.0689 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 172/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0536 - accuracy: 0.0097\n",
      "Epoch 172: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0536 - accuracy: 0.0097 - val_loss: 21.0785 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 173/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0668 - accuracy: 0.0097\n",
      "Epoch 173: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0668 - accuracy: 0.0097 - val_loss: 21.0862 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 174/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0741 - accuracy: 0.0097\n",
      "Epoch 174: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 21.0741 - accuracy: 0.0097 - val_loss: 21.0873 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 175/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0652 - accuracy: 0.0097\n",
      "Epoch 175: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 21.0653 - accuracy: 0.0097 - val_loss: 21.0819 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 176/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0619 - accuracy: 0.0086\n",
      "Epoch 176: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0621 - accuracy: 0.0085 - val_loss: 21.0824 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 177/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0721 - accuracy: 0.0099\n",
      "Epoch 177: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.0750 - accuracy: 0.0097 - val_loss: 21.0917 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 178/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0567 - accuracy: 0.0095\n",
      "Epoch 178: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0558 - accuracy: 0.0097 - val_loss: 21.0671 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 179/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0505 - accuracy: 0.0099\n",
      "Epoch 179: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0520 - accuracy: 0.0097 - val_loss: 21.0635 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 180/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0469 - accuracy: 0.0097\n",
      "Epoch 180: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0470 - accuracy: 0.0097 - val_loss: 21.0617 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 181/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0470 - accuracy: 0.0099\n",
      "Epoch 181: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0479 - accuracy: 0.0097 - val_loss: 21.0668 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 182/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0542 - accuracy: 0.0097\n",
      "Epoch 182: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0541 - accuracy: 0.0097 - val_loss: 21.0697 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 183/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0602 - accuracy: 0.0097\n",
      "Epoch 183: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0601 - accuracy: 0.0097 - val_loss: 21.0767 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 184/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0629 - accuracy: 0.0097\n",
      "Epoch 184: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0630 - accuracy: 0.0097 - val_loss: 21.0791 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 185/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0624 - accuracy: 0.0099\n",
      "Epoch 185: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0612 - accuracy: 0.0097 - val_loss: 21.0755 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 186/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0675 - accuracy: 0.0097\n",
      "Epoch 186: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0675 - accuracy: 0.0097 - val_loss: 21.0851 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 187/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0673 - accuracy: 0.0090\n",
      "Epoch 187: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0655 - accuracy: 0.0097 - val_loss: 21.0762 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 188/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0729 - accuracy: 0.0100\n",
      "Epoch 188: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0698 - accuracy: 0.0097 - val_loss: 21.0780 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 189/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0592 - accuracy: 0.0097\n",
      "Epoch 189: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0592 - accuracy: 0.0097 - val_loss: 21.0710 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 190/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0641 - accuracy: 0.0095\n",
      "Epoch 190: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0644 - accuracy: 0.0097 - val_loss: 21.0849 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 191/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0643 - accuracy: 0.0100\n",
      "Epoch 191: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0650 - accuracy: 0.0097 - val_loss: 21.0814 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 192/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0555 - accuracy: 0.0099\n",
      "Epoch 192: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0560 - accuracy: 0.0097 - val_loss: 21.0789 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 193/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0587 - accuracy: 0.0097\n",
      "Epoch 193: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0587 - accuracy: 0.0097 - val_loss: 21.0820 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 194/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0656 - accuracy: 0.0099\n",
      "Epoch 194: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0652 - accuracy: 0.0097 - val_loss: 21.0807 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 195/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0784 - accuracy: 0.0099\n",
      "Epoch 195: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 21.0786 - accuracy: 0.0097 - val_loss: 21.0888 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 196/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0722 - accuracy: 0.0097\n",
      "Epoch 196: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0719 - accuracy: 0.0097 - val_loss: 21.0851 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 197/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0730 - accuracy: 0.0096\n",
      "Epoch 197: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0725 - accuracy: 0.0097 - val_loss: 21.0931 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 198/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0647 - accuracy: 0.0100\n",
      "Epoch 198: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0635 - accuracy: 0.0097 - val_loss: 21.0843 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 199/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0694 - accuracy: 0.0096\n",
      "Epoch 199: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0690 - accuracy: 0.0097 - val_loss: 21.0880 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 200/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0855 - accuracy: 0.0097\n",
      "Epoch 200: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0855 - accuracy: 0.0097 - val_loss: 21.0995 - val_accuracy: 0.0086 - lr: 5.0000e-04\n",
      "Epoch 201/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0771 - accuracy: 0.0099\n",
      "Epoch 201: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0764 - accuracy: 0.0097 - val_loss: 21.0995 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 202/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0733 - accuracy: 0.0097\n",
      "Epoch 202: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0733 - accuracy: 0.0097 - val_loss: 21.0963 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 203/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0661 - accuracy: 0.0100\n",
      "Epoch 203: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0676 - accuracy: 0.0097 - val_loss: 21.0933 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 204/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0613 - accuracy: 0.0097\n",
      "Epoch 204: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0612 - accuracy: 0.0097 - val_loss: 21.0861 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 205/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0603 - accuracy: 0.0097\n",
      "Epoch 205: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0603 - accuracy: 0.0097 - val_loss: 21.0867 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 206/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0639 - accuracy: 0.0096\n",
      "Epoch 206: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0642 - accuracy: 0.0097 - val_loss: 21.0869 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 207/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0680 - accuracy: 0.0100\n",
      "Epoch 207: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0682 - accuracy: 0.0097 - val_loss: 21.0871 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 208/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0645 - accuracy: 0.0097\n",
      "Epoch 208: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0645 - accuracy: 0.0097 - val_loss: 21.0796 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 209/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0516 - accuracy: 0.0096\n",
      "Epoch 209: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0533 - accuracy: 0.0097 - val_loss: 21.0733 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 210/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0534 - accuracy: 0.0097\n",
      "Epoch 210: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0534 - accuracy: 0.0097 - val_loss: 21.0746 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 211/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0468 - accuracy: 0.0100\n",
      "Epoch 211: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0464 - accuracy: 0.0097 - val_loss: 21.0658 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 212/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0507 - accuracy: 0.0095\n",
      "Epoch 212: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0510 - accuracy: 0.0097 - val_loss: 21.0698 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 213/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0528 - accuracy: 0.0097\n",
      "Epoch 213: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 21.0525 - accuracy: 0.0097 - val_loss: 21.0729 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 214/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0590 - accuracy: 0.0096\n",
      "Epoch 214: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 21.0572 - accuracy: 0.0097 - val_loss: 21.0799 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 215/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0624 - accuracy: 0.0097\n",
      "Epoch 215: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0624 - accuracy: 0.0097 - val_loss: 21.0799 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 216/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0615 - accuracy: 0.0097\n",
      "Epoch 216: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0618 - accuracy: 0.0097 - val_loss: 21.0803 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 217/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0603 - accuracy: 0.0096\n",
      "Epoch 217: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0593 - accuracy: 0.0097 - val_loss: 21.0746 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 218/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0621 - accuracy: 0.0097\n",
      "Epoch 218: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0621 - accuracy: 0.0097 - val_loss: 21.0810 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 219/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0622 - accuracy: 0.0097\n",
      "Epoch 219: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0622 - accuracy: 0.0097 - val_loss: 21.0791 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 220/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0622 - accuracy: 0.0097\n",
      "Epoch 220: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0622 - accuracy: 0.0097 - val_loss: 21.0803 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 221/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0533 - accuracy: 0.0097\n",
      "Epoch 221: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0533 - accuracy: 0.0097 - val_loss: 21.0720 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 222/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0538 - accuracy: 0.0099\n",
      "Epoch 222: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0559 - accuracy: 0.0097 - val_loss: 21.0769 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 223/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0554 - accuracy: 0.0097\n",
      "Epoch 223: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0555 - accuracy: 0.0097 - val_loss: 21.0777 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 224/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0569 - accuracy: 0.0097\n",
      "Epoch 224: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0569 - accuracy: 0.0097 - val_loss: 21.0787 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 225/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0598 - accuracy: 0.0097\n",
      "Epoch 225: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0598 - accuracy: 0.0097 - val_loss: 21.0819 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 226/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0594 - accuracy: 0.0095\n",
      "Epoch 226: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0588 - accuracy: 0.0097 - val_loss: 21.0803 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 227/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0611 - accuracy: 0.0097\n",
      "Epoch 227: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0611 - accuracy: 0.0097 - val_loss: 21.0810 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 228/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0641 - accuracy: 0.0099\n",
      "Epoch 228: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.0633 - accuracy: 0.0097 - val_loss: 21.0832 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 229/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0663 - accuracy: 0.0099\n",
      "Epoch 229: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0654 - accuracy: 0.0097 - val_loss: 21.0840 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 230/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0711 - accuracy: 0.0099\n",
      "Epoch 230: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 21.0732 - accuracy: 0.0097 - val_loss: 21.0921 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 231/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0691 - accuracy: 0.0097\n",
      "Epoch 231: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0691 - accuracy: 0.0097 - val_loss: 21.0924 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 232/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0724 - accuracy: 0.0097\n",
      "Epoch 232: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0722 - accuracy: 0.0097 - val_loss: 21.0922 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 233/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0753 - accuracy: 0.0097\n",
      "Epoch 233: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0750 - accuracy: 0.0097 - val_loss: 21.0956 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 234/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0770 - accuracy: 0.0097\n",
      "Epoch 234: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0770 - accuracy: 0.0097 - val_loss: 21.0975 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 235/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0789 - accuracy: 0.0097\n",
      "Epoch 235: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0789 - accuracy: 0.0097 - val_loss: 21.0984 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 236/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0710 - accuracy: 0.0097\n",
      "Epoch 236: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0710 - accuracy: 0.0097 - val_loss: 21.0906 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 237/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0666 - accuracy: 0.0097\n",
      "Epoch 237: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0662 - accuracy: 0.0097 - val_loss: 21.0887 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 238/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0690 - accuracy: 0.0100\n",
      "Epoch 238: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0695 - accuracy: 0.0097 - val_loss: 21.0926 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 239/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0712 - accuracy: 0.0097\n",
      "Epoch 239: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0712 - accuracy: 0.0097 - val_loss: 21.0945 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 240/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0723 - accuracy: 0.0097\n",
      "Epoch 240: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0723 - accuracy: 0.0097 - val_loss: 21.0954 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 241/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0689 - accuracy: 0.0100\n",
      "Epoch 241: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0707 - accuracy: 0.0097 - val_loss: 21.0946 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 242/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0677 - accuracy: 0.0097\n",
      "Epoch 242: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0677 - accuracy: 0.0097 - val_loss: 21.0906 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 243/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0646 - accuracy: 0.0097\n",
      "Epoch 243: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0646 - accuracy: 0.0097 - val_loss: 21.0859 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 244/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0586 - accuracy: 0.0099\n",
      "Epoch 244: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0602 - accuracy: 0.0097 - val_loss: 21.0803 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 245/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0603 - accuracy: 0.0097\n",
      "Epoch 245: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 24ms/step - loss: 21.0603 - accuracy: 0.0097 - val_loss: 21.0828 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 246/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0626 - accuracy: 0.0097\n",
      "Epoch 246: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0626 - accuracy: 0.0097 - val_loss: 21.0852 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 247/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0621 - accuracy: 0.0097\n",
      "Epoch 247: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0621 - accuracy: 0.0097 - val_loss: 21.0862 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 248/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0654 - accuracy: 0.0095\n",
      "Epoch 248: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0647 - accuracy: 0.0097 - val_loss: 21.0878 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 249/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0632 - accuracy: 0.0099\n",
      "Epoch 249: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0624 - accuracy: 0.0097 - val_loss: 21.0867 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 250/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0649 - accuracy: 0.0097\n",
      "Epoch 250: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0648 - accuracy: 0.0097 - val_loss: 21.0878 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 251/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0667 - accuracy: 0.0097\n",
      "Epoch 251: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0669 - accuracy: 0.0097 - val_loss: 21.0911 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 252/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0652 - accuracy: 0.0097\n",
      "Epoch 252: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0652 - accuracy: 0.0097 - val_loss: 21.0920 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 253/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0710 - accuracy: 0.0097\n",
      "Epoch 253: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0710 - accuracy: 0.0097 - val_loss: 21.0966 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 254/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0677 - accuracy: 0.0097\n",
      "Epoch 254: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0677 - accuracy: 0.0097 - val_loss: 21.0933 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 255/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0691 - accuracy: 0.0097\n",
      "Epoch 255: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 21.0689 - accuracy: 0.0097 - val_loss: 21.0968 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 256/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0724 - accuracy: 0.0097\n",
      "Epoch 256: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 21.0724 - accuracy: 0.0097 - val_loss: 21.0996 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 257/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0668 - accuracy: 0.0097\n",
      "Epoch 257: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 24ms/step - loss: 21.0668 - accuracy: 0.0097 - val_loss: 21.0893 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 258/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0685 - accuracy: 0.0099\n",
      "Epoch 258: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 25ms/step - loss: 21.0678 - accuracy: 0.0097 - val_loss: 21.0935 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 259/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0707 - accuracy: 0.0099\n",
      "Epoch 259: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 21.0730 - accuracy: 0.0097 - val_loss: 21.0958 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 260/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0705 - accuracy: 0.0096\n",
      "Epoch 260: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0700 - accuracy: 0.0097 - val_loss: 21.0867 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 261/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0692 - accuracy: 0.0097\n",
      "Epoch 261: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 21.0692 - accuracy: 0.0097 - val_loss: 21.0863 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 262/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0700 - accuracy: 0.0097\n",
      "Epoch 262: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 21.0700 - accuracy: 0.0097 - val_loss: 21.0853 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 263/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0742 - accuracy: 0.0097\n",
      "Epoch 263: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 21.0740 - accuracy: 0.0097 - val_loss: 21.0902 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 264/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0744 - accuracy: 0.0097\n",
      "Epoch 264: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 21.0742 - accuracy: 0.0097 - val_loss: 21.0923 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 265/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0734 - accuracy: 0.0100\n",
      "Epoch 265: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 21.0738 - accuracy: 0.0097 - val_loss: 21.0921 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 266/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0751 - accuracy: 0.0099\n",
      "Epoch 266: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 21.0751 - accuracy: 0.0097 - val_loss: 21.0897 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 267/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0734 - accuracy: 0.0099\n",
      "Epoch 267: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 22ms/step - loss: 21.0719 - accuracy: 0.0097 - val_loss: 21.0880 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 268/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0733 - accuracy: 0.0095\n",
      "Epoch 268: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 26ms/step - loss: 21.0751 - accuracy: 0.0097 - val_loss: 21.0929 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 269/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0748 - accuracy: 0.0097\n",
      "Epoch 269: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 22ms/step - loss: 21.0751 - accuracy: 0.0097 - val_loss: 21.0921 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 270/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0666 - accuracy: 0.0097\n",
      "Epoch 270: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0666 - accuracy: 0.0097 - val_loss: 21.0867 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 271/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0642 - accuracy: 0.0095\n",
      "Epoch 271: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0659 - accuracy: 0.0097 - val_loss: 21.0895 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 272/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0645 - accuracy: 0.0099\n",
      "Epoch 272: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0652 - accuracy: 0.0097 - val_loss: 21.0859 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 273/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0558 - accuracy: 0.0096\n",
      "Epoch 273: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0567 - accuracy: 0.0097 - val_loss: 21.0813 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 274/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0543 - accuracy: 0.0097\n",
      "Epoch 274: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0543 - accuracy: 0.0097 - val_loss: 21.0767 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 275/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0543 - accuracy: 0.0097\n",
      "Epoch 275: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0544 - accuracy: 0.0097 - val_loss: 21.0798 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 276/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0522 - accuracy: 0.0097\n",
      "Epoch 276: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0517 - accuracy: 0.0097 - val_loss: 21.0807 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 277/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0583 - accuracy: 0.0099\n",
      "Epoch 277: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0597 - accuracy: 0.0097 - val_loss: 21.0899 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 278/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0556 - accuracy: 0.0099\n",
      "Epoch 278: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0550 - accuracy: 0.0097 - val_loss: 21.0844 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 279/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0563 - accuracy: 0.0100\n",
      "Epoch 279: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0541 - accuracy: 0.0097 - val_loss: 21.0851 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 280/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0526 - accuracy: 0.0097\n",
      "Epoch 280: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 21.0526 - accuracy: 0.0097 - val_loss: 21.0808 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 281/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0517 - accuracy: 0.0099\n",
      "Epoch 281: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0523 - accuracy: 0.0097 - val_loss: 21.0808 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 282/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0498 - accuracy: 0.0097\n",
      "Epoch 282: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0500 - accuracy: 0.0097 - val_loss: 21.0771 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 283/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0475 - accuracy: 0.0097\n",
      "Epoch 283: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0480 - accuracy: 0.0097 - val_loss: 21.0720 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 284/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0417 - accuracy: 0.0099\n",
      "Epoch 284: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0417 - accuracy: 0.0097 - val_loss: 21.0707 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 285/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0479 - accuracy: 0.0097\n",
      "Epoch 285: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0479 - accuracy: 0.0097 - val_loss: 21.0766 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 286/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0499 - accuracy: 0.0097\n",
      "Epoch 286: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0500 - accuracy: 0.0097 - val_loss: 21.0799 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 287/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0481 - accuracy: 0.0099\n",
      "Epoch 287: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 21.0496 - accuracy: 0.0097 - val_loss: 21.0797 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 288/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0479 - accuracy: 0.0099\n",
      "Epoch 288: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0476 - accuracy: 0.0097 - val_loss: 21.0795 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 289/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0478 - accuracy: 0.0095\n",
      "Epoch 289: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0474 - accuracy: 0.0097 - val_loss: 21.0780 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 290/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0524 - accuracy: 0.0096\n",
      "Epoch 290: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0532 - accuracy: 0.0097 - val_loss: 21.0829 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 291/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0529 - accuracy: 0.0097\n",
      "Epoch 291: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 21.0529 - accuracy: 0.0097 - val_loss: 21.0812 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 292/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0578 - accuracy: 0.0095\n",
      "Epoch 292: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 21.0576 - accuracy: 0.0097 - val_loss: 21.0859 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 293/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0552 - accuracy: 0.0100\n",
      "Epoch 293: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0567 - accuracy: 0.0097 - val_loss: 21.0863 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 294/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0584 - accuracy: 0.0097\n",
      "Epoch 294: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0580 - accuracy: 0.0097 - val_loss: 21.0872 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 295/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0663 - accuracy: 0.0097\n",
      "Epoch 295: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0661 - accuracy: 0.0097 - val_loss: 21.0929 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 296/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0658 - accuracy: 0.0099\n",
      "Epoch 296: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0647 - accuracy: 0.0097 - val_loss: 21.0902 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 297/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0662 - accuracy: 0.0097\n",
      "Epoch 297: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0662 - accuracy: 0.0097 - val_loss: 21.0941 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 298/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0654 - accuracy: 0.0099\n",
      "Epoch 298: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0637 - accuracy: 0.0097 - val_loss: 21.0908 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 299/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0604 - accuracy: 0.0095\n",
      "Epoch 299: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0598 - accuracy: 0.0097 - val_loss: 21.0894 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 300/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0568 - accuracy: 0.0097\n",
      "Epoch 300: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0568 - accuracy: 0.0097 - val_loss: 21.0865 - val_accuracy: 0.0086 - lr: 2.5000e-04\n",
      "Epoch 301/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0542 - accuracy: 0.0097\n",
      "Epoch 301: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 21.0542 - accuracy: 0.0097 - val_loss: 21.0850 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 302/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0550 - accuracy: 0.0097\n",
      "Epoch 302: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 21.0550 - accuracy: 0.0097 - val_loss: 21.0829 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 303/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0516 - accuracy: 0.0099\n",
      "Epoch 303: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0527 - accuracy: 0.0097 - val_loss: 21.0820 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 304/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0532 - accuracy: 0.0099\n",
      "Epoch 304: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0519 - accuracy: 0.0097 - val_loss: 21.0836 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 305/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0525 - accuracy: 0.0097\n",
      "Epoch 305: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0525 - accuracy: 0.0097 - val_loss: 21.0845 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 306/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0499 - accuracy: 0.0097\n",
      "Epoch 306: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0499 - accuracy: 0.0097 - val_loss: 21.0807 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 307/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0494 - accuracy: 0.0099\n",
      "Epoch 307: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0508 - accuracy: 0.0097 - val_loss: 21.0816 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 308/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0502 - accuracy: 0.0097\n",
      "Epoch 308: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0503 - accuracy: 0.0097 - val_loss: 21.0799 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 309/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0487 - accuracy: 0.0095\n",
      "Epoch 309: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0497 - accuracy: 0.0097 - val_loss: 21.0800 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 310/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0477 - accuracy: 0.0097\n",
      "Epoch 310: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0477 - accuracy: 0.0097 - val_loss: 21.0779 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 311/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0458 - accuracy: 0.0096\n",
      "Epoch 311: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 22ms/step - loss: 21.0484 - accuracy: 0.0097 - val_loss: 21.0801 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 312/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0488 - accuracy: 0.0097\n",
      "Epoch 312: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0490 - accuracy: 0.0097 - val_loss: 21.0797 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 313/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0493 - accuracy: 0.0096\n",
      "Epoch 313: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0490 - accuracy: 0.0097 - val_loss: 21.0821 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 314/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0542 - accuracy: 0.0097\n",
      "Epoch 314: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0541 - accuracy: 0.0097 - val_loss: 21.0856 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 315/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0551 - accuracy: 0.0099\n",
      "Epoch 315: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0548 - accuracy: 0.0097 - val_loss: 21.0846 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 316/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0526 - accuracy: 0.0100\n",
      "Epoch 316: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0534 - accuracy: 0.0097 - val_loss: 21.0861 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 317/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0509 - accuracy: 0.0097\n",
      "Epoch 317: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0509 - accuracy: 0.0097 - val_loss: 21.0845 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 318/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0524 - accuracy: 0.0100\n",
      "Epoch 318: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0501 - accuracy: 0.0097 - val_loss: 21.0822 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 319/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0510 - accuracy: 0.0097\n",
      "Epoch 319: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0510 - accuracy: 0.0097 - val_loss: 21.0838 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 320/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0503 - accuracy: 0.0099\n",
      "Epoch 320: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 21.0503 - accuracy: 0.0097 - val_loss: 21.0822 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 321/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0474 - accuracy: 0.0097\n",
      "Epoch 321: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 21.0474 - accuracy: 0.0097 - val_loss: 21.0799 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 322/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0468 - accuracy: 0.0100\n",
      "Epoch 322: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0473 - accuracy: 0.0097 - val_loss: 21.0777 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 323/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0482 - accuracy: 0.0093\n",
      "Epoch 323: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0479 - accuracy: 0.0097 - val_loss: 21.0782 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 324/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0524 - accuracy: 0.0097\n",
      "Epoch 324: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0521 - accuracy: 0.0097 - val_loss: 21.0813 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 325/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0536 - accuracy: 0.0097\n",
      "Epoch 325: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 21.0538 - accuracy: 0.0097 - val_loss: 21.0832 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 326/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0517 - accuracy: 0.0097\n",
      "Epoch 326: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 21.0523 - accuracy: 0.0097 - val_loss: 21.0812 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 327/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0482 - accuracy: 0.0099\n",
      "Epoch 327: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 22ms/step - loss: 21.0490 - accuracy: 0.0097 - val_loss: 21.0776 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 328/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0487 - accuracy: 0.0097\n",
      "Epoch 328: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 25ms/step - loss: 21.0485 - accuracy: 0.0097 - val_loss: 21.0781 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 329/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0504 - accuracy: 0.0097\n",
      "Epoch 329: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 27ms/step - loss: 21.0500 - accuracy: 0.0097 - val_loss: 21.0816 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 330/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0516 - accuracy: 0.0099\n",
      "Epoch 330: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 21.0537 - accuracy: 0.0097 - val_loss: 21.0840 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 331/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0520 - accuracy: 0.0097\n",
      "Epoch 331: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 21.0520 - accuracy: 0.0097 - val_loss: 21.0819 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 332/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0515 - accuracy: 0.0097\n",
      "Epoch 332: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0515 - accuracy: 0.0097 - val_loss: 21.0811 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 333/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0518 - accuracy: 0.0095\n",
      "Epoch 333: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0515 - accuracy: 0.0097 - val_loss: 21.0821 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 334/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0497 - accuracy: 0.0100\n",
      "Epoch 334: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0512 - accuracy: 0.0097 - val_loss: 21.0825 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 335/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0526 - accuracy: 0.0097\n",
      "Epoch 335: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0526 - accuracy: 0.0097 - val_loss: 21.0851 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 336/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0506 - accuracy: 0.0097\n",
      "Epoch 336: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 21.0507 - accuracy: 0.0097 - val_loss: 21.0832 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 337/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0505 - accuracy: 0.0095\n",
      "Epoch 337: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 21.0508 - accuracy: 0.0097 - val_loss: 21.0826 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 338/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0487 - accuracy: 0.0095\n",
      "Epoch 338: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0505 - accuracy: 0.0097 - val_loss: 21.0824 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 339/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0513 - accuracy: 0.0097\n",
      "Epoch 339: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0512 - accuracy: 0.0097 - val_loss: 21.0826 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 340/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0510 - accuracy: 0.0096\n",
      "Epoch 340: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0514 - accuracy: 0.0097 - val_loss: 21.0845 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 341/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0499 - accuracy: 0.0097\n",
      "Epoch 341: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0499 - accuracy: 0.0097 - val_loss: 21.0826 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 342/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0481 - accuracy: 0.0099\n",
      "Epoch 342: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0493 - accuracy: 0.0097 - val_loss: 21.0806 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 343/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0484 - accuracy: 0.0096\n",
      "Epoch 343: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0468 - accuracy: 0.0097 - val_loss: 21.0760 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 344/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0476 - accuracy: 0.0097\n",
      "Epoch 344: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0476 - accuracy: 0.0097 - val_loss: 21.0765 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 345/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0471 - accuracy: 0.0097\n",
      "Epoch 345: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 22ms/step - loss: 21.0471 - accuracy: 0.0097 - val_loss: 21.0761 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 346/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0476 - accuracy: 0.0099\n",
      "Epoch 346: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 21.0472 - accuracy: 0.0097 - val_loss: 21.0773 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 347/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0477 - accuracy: 0.0099\n",
      "Epoch 347: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0470 - accuracy: 0.0097 - val_loss: 21.0767 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 348/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0492 - accuracy: 0.0099\n",
      "Epoch 348: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0476 - accuracy: 0.0097 - val_loss: 21.0769 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 349/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0481 - accuracy: 0.0097\n",
      "Epoch 349: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0481 - accuracy: 0.0097 - val_loss: 21.0783 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 350/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0495 - accuracy: 0.0097\n",
      "Epoch 350: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0495 - accuracy: 0.0097 - val_loss: 21.0791 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 351/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0498 - accuracy: 0.0100\n",
      "Epoch 351: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0508 - accuracy: 0.0097 - val_loss: 21.0818 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 352/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0520 - accuracy: 0.0097\n",
      "Epoch 352: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 21.0520 - accuracy: 0.0097 - val_loss: 21.0815 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 353/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0509 - accuracy: 0.0097\n",
      "Epoch 353: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 21.0511 - accuracy: 0.0097 - val_loss: 21.0809 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 354/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0510 - accuracy: 0.0099\n",
      "Epoch 354: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0492 - accuracy: 0.0097 - val_loss: 21.0772 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 355/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0487 - accuracy: 0.0095\n",
      "Epoch 355: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0496 - accuracy: 0.0097 - val_loss: 21.0767 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 356/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0506 - accuracy: 0.0096\n",
      "Epoch 356: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0480 - accuracy: 0.0097 - val_loss: 21.0739 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 357/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0524 - accuracy: 0.0096\n",
      "Epoch 357: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0495 - accuracy: 0.0097 - val_loss: 21.0766 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 358/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0543 - accuracy: 0.0097\n",
      "Epoch 358: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0544 - accuracy: 0.0097 - val_loss: 21.0804 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 359/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0538 - accuracy: 0.0096\n",
      "Epoch 359: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0533 - accuracy: 0.0097 - val_loss: 21.0791 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 360/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0532 - accuracy: 0.0097\n",
      "Epoch 360: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 21.0535 - accuracy: 0.0097 - val_loss: 21.0785 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 361/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0513 - accuracy: 0.0097\n",
      "Epoch 361: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 21.0510 - accuracy: 0.0097 - val_loss: 21.0759 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 362/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0528 - accuracy: 0.0100\n",
      "Epoch 362: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0525 - accuracy: 0.0097 - val_loss: 21.0787 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 363/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0547 - accuracy: 0.0097\n",
      "Epoch 363: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0547 - accuracy: 0.0097 - val_loss: 21.0818 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 364/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0530 - accuracy: 0.0099\n",
      "Epoch 364: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0540 - accuracy: 0.0097 - val_loss: 21.0817 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 365/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0586 - accuracy: 0.0097\n",
      "Epoch 365: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0581 - accuracy: 0.0097 - val_loss: 21.0867 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 366/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0613 - accuracy: 0.0095\n",
      "Epoch 366: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0606 - accuracy: 0.0097 - val_loss: 21.0898 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 367/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0607 - accuracy: 0.0097\n",
      "Epoch 367: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0607 - accuracy: 0.0097 - val_loss: 21.0886 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 368/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0596 - accuracy: 0.0097\n",
      "Epoch 368: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 21.0599 - accuracy: 0.0097 - val_loss: 21.0865 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 369/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0559 - accuracy: 0.0097\n",
      "Epoch 369: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0562 - accuracy: 0.0097 - val_loss: 21.0861 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 370/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0522 - accuracy: 0.0099\n",
      "Epoch 370: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0523 - accuracy: 0.0097 - val_loss: 21.0823 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 371/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0515 - accuracy: 0.0097\n",
      "Epoch 371: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 21.0515 - accuracy: 0.0097 - val_loss: 21.0814 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 372/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0512 - accuracy: 0.0097\n",
      "Epoch 372: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0511 - accuracy: 0.0097 - val_loss: 21.0827 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 373/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0507 - accuracy: 0.0097\n",
      "Epoch 373: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0507 - accuracy: 0.0097 - val_loss: 21.0818 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 374/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0487 - accuracy: 0.0092\n",
      "Epoch 374: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0469 - accuracy: 0.0097 - val_loss: 21.0757 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 375/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0487 - accuracy: 0.0097\n",
      "Epoch 375: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 28ms/step - loss: 21.0487 - accuracy: 0.0097 - val_loss: 21.0786 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 376/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0479 - accuracy: 0.0097\n",
      "Epoch 376: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 25ms/step - loss: 21.0479 - accuracy: 0.0097 - val_loss: 21.0767 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 377/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0483 - accuracy: 0.0097\n",
      "Epoch 377: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 21.0483 - accuracy: 0.0097 - val_loss: 21.0772 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 378/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0469 - accuracy: 0.0097\n",
      "Epoch 378: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 21.0469 - accuracy: 0.0097 - val_loss: 21.0744 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 379/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0474 - accuracy: 0.0097\n",
      "Epoch 379: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 21.0477 - accuracy: 0.0097 - val_loss: 21.0755 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 380/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0466 - accuracy: 0.0097\n",
      "Epoch 380: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 21.0466 - accuracy: 0.0097 - val_loss: 21.0737 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 381/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0488 - accuracy: 0.0097\n",
      "Epoch 381: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 30ms/step - loss: 21.0488 - accuracy: 0.0097 - val_loss: 21.0798 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 382/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0497 - accuracy: 0.0095\n",
      "Epoch 382: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 21.0504 - accuracy: 0.0097 - val_loss: 21.0816 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 383/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0497 - accuracy: 0.0095\n",
      "Epoch 383: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0513 - accuracy: 0.0097 - val_loss: 21.0816 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 384/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0488 - accuracy: 0.0097\n",
      "Epoch 384: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0488 - accuracy: 0.0097 - val_loss: 21.0795 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 385/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0498 - accuracy: 0.0097\n",
      "Epoch 385: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0498 - accuracy: 0.0097 - val_loss: 21.0817 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 386/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0509 - accuracy: 0.0096\n",
      "Epoch 386: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0506 - accuracy: 0.0097 - val_loss: 21.0816 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 387/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0514 - accuracy: 0.0097\n",
      "Epoch 387: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 21.0511 - accuracy: 0.0097 - val_loss: 21.0847 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 388/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0527 - accuracy: 0.0097\n",
      "Epoch 388: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 21.0526 - accuracy: 0.0097 - val_loss: 21.0870 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 389/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0537 - accuracy: 0.0100\n",
      "Epoch 389: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 21.0534 - accuracy: 0.0097 - val_loss: 21.0859 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 390/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0489 - accuracy: 0.0096\n",
      "Epoch 390: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0512 - accuracy: 0.0097 - val_loss: 21.0852 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 391/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0497 - accuracy: 0.0097\n",
      "Epoch 391: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0497 - accuracy: 0.0097 - val_loss: 21.0821 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 392/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0498 - accuracy: 0.0099\n",
      "Epoch 392: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 21.0486 - accuracy: 0.0097 - val_loss: 21.0804 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 393/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0481 - accuracy: 0.0099\n",
      "Epoch 393: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0489 - accuracy: 0.0097 - val_loss: 21.0806 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 394/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0488 - accuracy: 0.0097\n",
      "Epoch 394: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 24ms/step - loss: 21.0497 - accuracy: 0.0097 - val_loss: 21.0797 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 395/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 21.0461 - accuracy: 0.0096\n",
      "Epoch 395: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 21.0463 - accuracy: 0.0097 - val_loss: 21.0764 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 396/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0480 - accuracy: 0.0099\n",
      "Epoch 396: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 21.0480 - accuracy: 0.0097 - val_loss: 21.0792 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 397/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0542 - accuracy: 0.0097\n",
      "Epoch 397: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 21.0542 - accuracy: 0.0097 - val_loss: 21.0860 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 398/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 21.0569 - accuracy: 0.0097\n",
      "Epoch 398: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0569 - accuracy: 0.0097 - val_loss: 21.0883 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 399/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 21.0599 - accuracy: 0.0099\n",
      "Epoch 399: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 21.0599 - accuracy: 0.0097 - val_loss: 21.0915 - val_accuracy: 0.0086 - lr: 1.2500e-04\n",
      "Epoch 400/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 21.0561 - accuracy: 0.0097\n",
      "Epoch 400: val_accuracy did not improve from 0.00857\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 21.0559 - accuracy: 0.0097 - val_loss: 21.0875 - val_accuracy: 0.0086 - lr: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "EPOCHS = 400\n",
    "NetNAME = 'LSTM2_lr100'\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "NAME = timestamp + NetNAME +\"_batchsize\";print(NAME)\n",
    "logdir = \"./logs/\" + NAME\n",
    "modeldir = \"./model/\"+NAME+\".h5\"\n",
    "\n",
    "# 不调整学习率会比较好\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch % 100 == 0 and epoch != 0:\n",
    "        lr = lr / 2\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=modeldir, monitor='val_accuracy', verbose=1, save_best_only=True, mode = 'max')\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(\n",
    "        x=train_dataset, \n",
    "        epochs=EPOCHS,     \n",
    "        validation_data=test_dataset,    \n",
    "        callbacks=[tensorboard_callback,checkpoint])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d0300-f0c4-42bb-b432-db57a7c0fec8",
   "metadata": {},
   "source": [
    "## 蓝球模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ec2e98aa-6912-4df0-8754-ed090ab1616c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20231204-192802B_LSTM2_batchsize\n",
      "Epoch 1/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.3152 - accuracy: 0.0604\n",
      "Epoch 1: val_accuracy improved from -inf to 0.05353, saving model to ./model\\20231204-192802B_LSTM2_batchsize.h5\n",
      "78/78 [==============================] - 4s 27ms/step - loss: 0.3152 - accuracy: 0.0604 - val_loss: 0.2348 - val_accuracy: 0.0535\n",
      "Epoch 2/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2347 - accuracy: 0.0567\n",
      "Epoch 2: val_accuracy did not improve from 0.05353\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2346 - accuracy: 0.0580 - val_loss: 0.2345 - val_accuracy: 0.0535\n",
      "Epoch 3/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2348 - accuracy: 0.0584\n",
      "Epoch 3: val_accuracy did not improve from 0.05353\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2347 - accuracy: 0.0588 - val_loss: 0.2347 - val_accuracy: 0.0535\n",
      "Epoch 4/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2348 - accuracy: 0.0608\n",
      "Epoch 4: val_accuracy did not improve from 0.05353\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2347 - accuracy: 0.0604 - val_loss: 0.2343 - val_accuracy: 0.0535\n",
      "Epoch 5/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2346 - accuracy: 0.0679\n",
      "Epoch 5: val_accuracy did not improve from 0.05353\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2346 - accuracy: 0.0677 - val_loss: 0.2345 - val_accuracy: 0.0535\n",
      "Epoch 6/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2348 - accuracy: 0.0575\n",
      "Epoch 6: val_accuracy improved from 0.05353 to 0.07281, saving model to ./model\\20231204-192802B_LSTM2_batchsize.h5\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2348 - accuracy: 0.0572 - val_loss: 0.2343 - val_accuracy: 0.0728\n",
      "Epoch 7/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2346 - accuracy: 0.0612\n",
      "Epoch 7: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2346 - accuracy: 0.0612 - val_loss: 0.2345 - val_accuracy: 0.0471\n",
      "Epoch 8/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2348 - accuracy: 0.0588\n",
      "Epoch 8: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2348 - accuracy: 0.0588 - val_loss: 0.2346 - val_accuracy: 0.0535\n",
      "Epoch 9/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2346 - accuracy: 0.0608\n",
      "Epoch 9: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2346 - accuracy: 0.0608 - val_loss: 0.2343 - val_accuracy: 0.0535\n",
      "Epoch 10/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2346 - accuracy: 0.0646\n",
      "Epoch 10: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2347 - accuracy: 0.0641 - val_loss: 0.2345 - val_accuracy: 0.0535\n",
      "Epoch 11/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2345 - accuracy: 0.0650\n",
      "Epoch 11: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2345 - accuracy: 0.0653 - val_loss: 0.2346 - val_accuracy: 0.0535\n",
      "Epoch 12/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2345 - accuracy: 0.0613\n",
      "Epoch 12: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2345 - accuracy: 0.0612 - val_loss: 0.2345 - val_accuracy: 0.0535\n",
      "Epoch 13/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2348 - accuracy: 0.0669\n",
      "Epoch 13: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2348 - accuracy: 0.0669 - val_loss: 0.2343 - val_accuracy: 0.0471\n",
      "Epoch 14/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2345 - accuracy: 0.0613\n",
      "Epoch 14: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2345 - accuracy: 0.0629 - val_loss: 0.2342 - val_accuracy: 0.0535\n",
      "Epoch 15/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2346 - accuracy: 0.0678\n",
      "Epoch 15: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2345 - accuracy: 0.0673 - val_loss: 0.2344 - val_accuracy: 0.0535\n",
      "Epoch 16/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2346 - accuracy: 0.0624\n",
      "Epoch 16: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2346 - accuracy: 0.0624 - val_loss: 0.2346 - val_accuracy: 0.0471\n",
      "Epoch 17/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2345 - accuracy: 0.0650\n",
      "Epoch 17: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2345 - accuracy: 0.0641 - val_loss: 0.2344 - val_accuracy: 0.0535\n",
      "Epoch 18/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2347 - accuracy: 0.0621\n",
      "Epoch 18: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2347 - accuracy: 0.0624 - val_loss: 0.2343 - val_accuracy: 0.0471\n",
      "Epoch 19/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2345 - accuracy: 0.0579\n",
      "Epoch 19: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2345 - accuracy: 0.0588 - val_loss: 0.2345 - val_accuracy: 0.0535\n",
      "Epoch 20/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2348 - accuracy: 0.0575\n",
      "Epoch 20: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2348 - accuracy: 0.0572 - val_loss: 0.2346 - val_accuracy: 0.0471\n",
      "Epoch 21/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2345 - accuracy: 0.0604\n",
      "Epoch 21: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2345 - accuracy: 0.0596 - val_loss: 0.2344 - val_accuracy: 0.0535\n",
      "Epoch 22/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2345 - accuracy: 0.0641\n",
      "Epoch 22: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2345 - accuracy: 0.0641 - val_loss: 0.2343 - val_accuracy: 0.0535\n",
      "Epoch 23/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2346 - accuracy: 0.0576\n",
      "Epoch 23: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2346 - accuracy: 0.0576 - val_loss: 0.2346 - val_accuracy: 0.0471\n",
      "Epoch 24/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2344 - accuracy: 0.0576\n",
      "Epoch 24: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2344 - accuracy: 0.0576 - val_loss: 0.2345 - val_accuracy: 0.0450\n",
      "Epoch 25/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2344 - accuracy: 0.0637\n",
      "Epoch 25: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2344 - accuracy: 0.0641 - val_loss: 0.2344 - val_accuracy: 0.0471\n",
      "Epoch 26/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2345 - accuracy: 0.0608\n",
      "Epoch 26: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2345 - accuracy: 0.0608 - val_loss: 0.2346 - val_accuracy: 0.0535\n",
      "Epoch 27/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2344 - accuracy: 0.0600\n",
      "Epoch 27: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2344 - accuracy: 0.0604 - val_loss: 0.2344 - val_accuracy: 0.0471\n",
      "Epoch 28/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2344 - accuracy: 0.0567\n",
      "Epoch 28: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2344 - accuracy: 0.0580 - val_loss: 0.2345 - val_accuracy: 0.0450\n",
      "Epoch 29/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2344 - accuracy: 0.0613\n",
      "Epoch 29: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2344 - accuracy: 0.0616 - val_loss: 0.2345 - val_accuracy: 0.0535\n",
      "Epoch 30/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2346 - accuracy: 0.0617\n",
      "Epoch 30: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2346 - accuracy: 0.0612 - val_loss: 0.2345 - val_accuracy: 0.0471\n",
      "Epoch 31/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2346 - accuracy: 0.0658\n",
      "Epoch 31: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2345 - accuracy: 0.0649 - val_loss: 0.2344 - val_accuracy: 0.0407\n",
      "Epoch 32/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2344 - accuracy: 0.0637\n",
      "Epoch 32: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2344 - accuracy: 0.0637 - val_loss: 0.2346 - val_accuracy: 0.0535\n",
      "Epoch 33/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2345 - accuracy: 0.0609\n",
      "Epoch 33: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2345 - accuracy: 0.0608 - val_loss: 0.2343 - val_accuracy: 0.0578\n",
      "Epoch 34/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2343 - accuracy: 0.0616\n",
      "Epoch 34: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2343 - accuracy: 0.0616 - val_loss: 0.2345 - val_accuracy: 0.0428\n",
      "Epoch 35/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2343 - accuracy: 0.0625\n",
      "Epoch 35: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2342 - accuracy: 0.0629 - val_loss: 0.2349 - val_accuracy: 0.0471\n",
      "Epoch 36/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2345 - accuracy: 0.0625\n",
      "Epoch 36: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2345 - accuracy: 0.0624 - val_loss: 0.2345 - val_accuracy: 0.0471\n",
      "Epoch 37/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2343 - accuracy: 0.0641\n",
      "Epoch 37: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2343 - accuracy: 0.0645 - val_loss: 0.2348 - val_accuracy: 0.0471\n",
      "Epoch 38/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2343 - accuracy: 0.0661\n",
      "Epoch 38: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2343 - accuracy: 0.0661 - val_loss: 0.2342 - val_accuracy: 0.0407\n",
      "Epoch 39/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2342 - accuracy: 0.0662\n",
      "Epoch 39: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 0.2342 - accuracy: 0.0661 - val_loss: 0.2348 - val_accuracy: 0.0514\n",
      "Epoch 40/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2340 - accuracy: 0.0692\n",
      "Epoch 40: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.2340 - accuracy: 0.0697 - val_loss: 0.2346 - val_accuracy: 0.0514\n",
      "Epoch 41/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2339 - accuracy: 0.0703\n",
      "Epoch 41: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2339 - accuracy: 0.0710 - val_loss: 0.2347 - val_accuracy: 0.0493\n",
      "Epoch 42/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2338 - accuracy: 0.0734\n",
      "Epoch 42: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2338 - accuracy: 0.0734 - val_loss: 0.2344 - val_accuracy: 0.0514\n",
      "Epoch 43/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2339 - accuracy: 0.0704\n",
      "Epoch 43: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2338 - accuracy: 0.0718 - val_loss: 0.2344 - val_accuracy: 0.0514\n",
      "Epoch 44/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2337 - accuracy: 0.0771\n",
      "Epoch 44: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 0.2337 - accuracy: 0.0766 - val_loss: 0.2346 - val_accuracy: 0.0514\n",
      "Epoch 45/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2334 - accuracy: 0.0762\n",
      "Epoch 45: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2334 - accuracy: 0.0762 - val_loss: 0.2347 - val_accuracy: 0.0450\n",
      "Epoch 46/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2333 - accuracy: 0.0808\n",
      "Epoch 46: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2333 - accuracy: 0.0815 - val_loss: 0.2347 - val_accuracy: 0.0471\n",
      "Epoch 47/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2332 - accuracy: 0.0785\n",
      "Epoch 47: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2332 - accuracy: 0.0787 - val_loss: 0.2350 - val_accuracy: 0.0450\n",
      "Epoch 48/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2332 - accuracy: 0.0743\n",
      "Epoch 48: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2332 - accuracy: 0.0742 - val_loss: 0.2350 - val_accuracy: 0.0450\n",
      "Epoch 49/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2330 - accuracy: 0.0769\n",
      "Epoch 49: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2329 - accuracy: 0.0787 - val_loss: 0.2352 - val_accuracy: 0.0471\n",
      "Epoch 50/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2329 - accuracy: 0.0858\n",
      "Epoch 50: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2329 - accuracy: 0.0852 - val_loss: 0.2353 - val_accuracy: 0.0514\n",
      "Epoch 51/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2326 - accuracy: 0.0833\n",
      "Epoch 51: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2326 - accuracy: 0.0839 - val_loss: 0.2354 - val_accuracy: 0.0407\n",
      "Epoch 52/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2324 - accuracy: 0.0916\n",
      "Epoch 52: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2324 - accuracy: 0.0916 - val_loss: 0.2355 - val_accuracy: 0.0514\n",
      "Epoch 53/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2322 - accuracy: 0.0864\n",
      "Epoch 53: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2323 - accuracy: 0.0864 - val_loss: 0.2357 - val_accuracy: 0.0471\n",
      "Epoch 54/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2323 - accuracy: 0.0946\n",
      "Epoch 54: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2323 - accuracy: 0.0941 - val_loss: 0.2358 - val_accuracy: 0.0514\n",
      "Epoch 55/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2321 - accuracy: 0.0850\n",
      "Epoch 55: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2321 - accuracy: 0.0856 - val_loss: 0.2359 - val_accuracy: 0.0557\n",
      "Epoch 56/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2320 - accuracy: 0.0950\n",
      "Epoch 56: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2320 - accuracy: 0.0949 - val_loss: 0.2359 - val_accuracy: 0.0557\n",
      "Epoch 57/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2317 - accuracy: 0.0900\n",
      "Epoch 57: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2316 - accuracy: 0.0904 - val_loss: 0.2367 - val_accuracy: 0.0578\n",
      "Epoch 58/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2317 - accuracy: 0.0962\n",
      "Epoch 58: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2317 - accuracy: 0.0953 - val_loss: 0.2364 - val_accuracy: 0.0514\n",
      "Epoch 59/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2314 - accuracy: 0.0905\n",
      "Epoch 59: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2314 - accuracy: 0.0916 - val_loss: 0.2367 - val_accuracy: 0.0557\n",
      "Epoch 60/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2313 - accuracy: 0.0942\n",
      "Epoch 60: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2313 - accuracy: 0.0945 - val_loss: 0.2369 - val_accuracy: 0.0493\n",
      "Epoch 61/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2311 - accuracy: 0.0979\n",
      "Epoch 61: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2311 - accuracy: 0.0981 - val_loss: 0.2370 - val_accuracy: 0.0535\n",
      "Epoch 62/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2307 - accuracy: 0.0974\n",
      "Epoch 62: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2307 - accuracy: 0.0973 - val_loss: 0.2372 - val_accuracy: 0.0471\n",
      "Epoch 63/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2307 - accuracy: 0.1046\n",
      "Epoch 63: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2307 - accuracy: 0.1046 - val_loss: 0.2374 - val_accuracy: 0.0514\n",
      "Epoch 64/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2305 - accuracy: 0.0978\n",
      "Epoch 64: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2305 - accuracy: 0.0977 - val_loss: 0.2375 - val_accuracy: 0.0621\n",
      "Epoch 65/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2303 - accuracy: 0.1049\n",
      "Epoch 65: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2303 - accuracy: 0.1050 - val_loss: 0.2376 - val_accuracy: 0.0493\n",
      "Epoch 66/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2301 - accuracy: 0.1026\n",
      "Epoch 66: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2301 - accuracy: 0.1026 - val_loss: 0.2379 - val_accuracy: 0.0471\n",
      "Epoch 67/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2298 - accuracy: 0.1084\n",
      "Epoch 67: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2298 - accuracy: 0.1087 - val_loss: 0.2383 - val_accuracy: 0.0557\n",
      "Epoch 68/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2297 - accuracy: 0.1094\n",
      "Epoch 68: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2298 - accuracy: 0.1091 - val_loss: 0.2384 - val_accuracy: 0.0471\n",
      "Epoch 69/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2296 - accuracy: 0.1092\n",
      "Epoch 69: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2296 - accuracy: 0.1091 - val_loss: 0.2383 - val_accuracy: 0.0493\n",
      "Epoch 70/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2293 - accuracy: 0.1069\n",
      "Epoch 70: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2293 - accuracy: 0.1075 - val_loss: 0.2384 - val_accuracy: 0.0535\n",
      "Epoch 71/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2291 - accuracy: 0.1136\n",
      "Epoch 71: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2291 - accuracy: 0.1135 - val_loss: 0.2382 - val_accuracy: 0.0535\n",
      "Epoch 72/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2288 - accuracy: 0.1169\n",
      "Epoch 72: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2288 - accuracy: 0.1172 - val_loss: 0.2389 - val_accuracy: 0.0428\n",
      "Epoch 73/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2284 - accuracy: 0.1153\n",
      "Epoch 73: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 0.2284 - accuracy: 0.1152 - val_loss: 0.2389 - val_accuracy: 0.0493\n",
      "Epoch 74/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2283 - accuracy: 0.1168\n",
      "Epoch 74: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2282 - accuracy: 0.1176 - val_loss: 0.2393 - val_accuracy: 0.0428\n",
      "Epoch 75/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2282 - accuracy: 0.1192\n",
      "Epoch 75: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2282 - accuracy: 0.1192 - val_loss: 0.2394 - val_accuracy: 0.0557\n",
      "Epoch 76/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2276 - accuracy: 0.1269\n",
      "Epoch 76: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.2276 - accuracy: 0.1269 - val_loss: 0.2399 - val_accuracy: 0.0407\n",
      "Epoch 77/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2277 - accuracy: 0.1184\n",
      "Epoch 77: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2277 - accuracy: 0.1192 - val_loss: 0.2399 - val_accuracy: 0.0385\n",
      "Epoch 78/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2273 - accuracy: 0.1196\n",
      "Epoch 78: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2273 - accuracy: 0.1196 - val_loss: 0.2405 - val_accuracy: 0.0364\n",
      "Epoch 79/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2270 - accuracy: 0.1296\n",
      "Epoch 79: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2269 - accuracy: 0.1298 - val_loss: 0.2403 - val_accuracy: 0.0514\n",
      "Epoch 80/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2266 - accuracy: 0.1319\n",
      "Epoch 80: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2266 - accuracy: 0.1318 - val_loss: 0.2410 - val_accuracy: 0.0493\n",
      "Epoch 81/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2263 - accuracy: 0.1296\n",
      "Epoch 81: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2263 - accuracy: 0.1298 - val_loss: 0.2409 - val_accuracy: 0.0428\n",
      "Epoch 82/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2258 - accuracy: 0.1307\n",
      "Epoch 82: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2258 - accuracy: 0.1306 - val_loss: 0.2411 - val_accuracy: 0.0535\n",
      "Epoch 83/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2257 - accuracy: 0.1334\n",
      "Epoch 83: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.2257 - accuracy: 0.1334 - val_loss: 0.2420 - val_accuracy: 0.0535\n",
      "Epoch 84/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2252 - accuracy: 0.1368\n",
      "Epoch 84: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2252 - accuracy: 0.1367 - val_loss: 0.2421 - val_accuracy: 0.0557\n",
      "Epoch 85/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2248 - accuracy: 0.1429\n",
      "Epoch 85: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.2248 - accuracy: 0.1427 - val_loss: 0.2423 - val_accuracy: 0.0471\n",
      "Epoch 86/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2250 - accuracy: 0.1472\n",
      "Epoch 86: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2250 - accuracy: 0.1472 - val_loss: 0.2423 - val_accuracy: 0.0557\n",
      "Epoch 87/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2243 - accuracy: 0.1492\n",
      "Epoch 87: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2243 - accuracy: 0.1492 - val_loss: 0.2430 - val_accuracy: 0.0450\n",
      "Epoch 88/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2240 - accuracy: 0.1505\n",
      "Epoch 88: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 0.2241 - accuracy: 0.1500 - val_loss: 0.2440 - val_accuracy: 0.0471\n",
      "Epoch 89/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2237 - accuracy: 0.1530\n",
      "Epoch 89: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2237 - accuracy: 0.1533 - val_loss: 0.2452 - val_accuracy: 0.0471\n",
      "Epoch 90/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2238 - accuracy: 0.1404\n",
      "Epoch 90: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2237 - accuracy: 0.1415 - val_loss: 0.2451 - val_accuracy: 0.0364\n",
      "Epoch 91/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2226 - accuracy: 0.1538\n",
      "Epoch 91: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2225 - accuracy: 0.1533 - val_loss: 0.2460 - val_accuracy: 0.0364\n",
      "Epoch 92/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.1468\n",
      "Epoch 92: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 2s 29ms/step - loss: 0.2228 - accuracy: 0.1468 - val_loss: 0.2460 - val_accuracy: 0.0385\n",
      "Epoch 93/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2223 - accuracy: 0.1579\n",
      "Epoch 93: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.2222 - accuracy: 0.1582 - val_loss: 0.2462 - val_accuracy: 0.0428\n",
      "Epoch 94/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2223 - accuracy: 0.1492\n",
      "Epoch 94: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2223 - accuracy: 0.1492 - val_loss: 0.2463 - val_accuracy: 0.0428\n",
      "Epoch 95/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2216 - accuracy: 0.1612\n",
      "Epoch 95: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2216 - accuracy: 0.1622 - val_loss: 0.2470 - val_accuracy: 0.0407\n",
      "Epoch 96/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2210 - accuracy: 0.1678\n",
      "Epoch 96: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2209 - accuracy: 0.1695 - val_loss: 0.2473 - val_accuracy: 0.0428\n",
      "Epoch 97/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2207 - accuracy: 0.1709\n",
      "Epoch 97: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.2207 - accuracy: 0.1707 - val_loss: 0.2475 - val_accuracy: 0.0385\n",
      "Epoch 98/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2206 - accuracy: 0.1604\n",
      "Epoch 98: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 2s 22ms/step - loss: 0.2205 - accuracy: 0.1610 - val_loss: 0.2482 - val_accuracy: 0.0493\n",
      "Epoch 99/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2198 - accuracy: 0.1700\n",
      "Epoch 99: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2198 - accuracy: 0.1703 - val_loss: 0.2484 - val_accuracy: 0.0514\n",
      "Epoch 100/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2192 - accuracy: 0.1706\n",
      "Epoch 100: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2191 - accuracy: 0.1723 - val_loss: 0.2501 - val_accuracy: 0.0514\n",
      "Epoch 101/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2191 - accuracy: 0.1663\n",
      "Epoch 101: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2191 - accuracy: 0.1671 - val_loss: 0.2495 - val_accuracy: 0.0514\n",
      "Epoch 102/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2196 - accuracy: 0.1721\n",
      "Epoch 102: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2196 - accuracy: 0.1719 - val_loss: 0.2490 - val_accuracy: 0.0535\n",
      "Epoch 103/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2178 - accuracy: 0.1813\n",
      "Epoch 103: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2177 - accuracy: 0.1813 - val_loss: 0.2506 - val_accuracy: 0.0600\n",
      "Epoch 104/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2175 - accuracy: 0.1801\n",
      "Epoch 104: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2173 - accuracy: 0.1813 - val_loss: 0.2515 - val_accuracy: 0.0621\n",
      "Epoch 105/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2168 - accuracy: 0.1789\n",
      "Epoch 105: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2168 - accuracy: 0.1792 - val_loss: 0.2510 - val_accuracy: 0.0621\n",
      "Epoch 106/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2165 - accuracy: 0.1930\n",
      "Epoch 106: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2165 - accuracy: 0.1930 - val_loss: 0.2515 - val_accuracy: 0.0493\n",
      "Epoch 107/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2158 - accuracy: 0.1883\n",
      "Epoch 107: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2158 - accuracy: 0.1882 - val_loss: 0.2511 - val_accuracy: 0.0600\n",
      "Epoch 108/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2152 - accuracy: 0.1971\n",
      "Epoch 108: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.2152 - accuracy: 0.1967 - val_loss: 0.2526 - val_accuracy: 0.0535\n",
      "Epoch 109/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2152 - accuracy: 0.1933\n",
      "Epoch 109: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2152 - accuracy: 0.1930 - val_loss: 0.2525 - val_accuracy: 0.0535\n",
      "Epoch 110/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2148 - accuracy: 0.1983\n",
      "Epoch 110: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.2148 - accuracy: 0.1983 - val_loss: 0.2522 - val_accuracy: 0.0600\n",
      "Epoch 111/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2139 - accuracy: 0.1970\n",
      "Epoch 111: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2140 - accuracy: 0.1979 - val_loss: 0.2537 - val_accuracy: 0.0493\n",
      "Epoch 112/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2137 - accuracy: 0.2119\n",
      "Epoch 112: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2137 - accuracy: 0.2117 - val_loss: 0.2536 - val_accuracy: 0.0557\n",
      "Epoch 113/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.2109\n",
      "Epoch 113: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2125 - accuracy: 0.2109 - val_loss: 0.2536 - val_accuracy: 0.0600\n",
      "Epoch 114/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2123 - accuracy: 0.2150\n",
      "Epoch 114: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2122 - accuracy: 0.2165 - val_loss: 0.2550 - val_accuracy: 0.0600\n",
      "Epoch 115/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2116 - accuracy: 0.2183\n",
      "Epoch 115: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2115 - accuracy: 0.2214 - val_loss: 0.2541 - val_accuracy: 0.0578\n",
      "Epoch 116/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2116 - accuracy: 0.2163\n",
      "Epoch 116: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2114 - accuracy: 0.2178 - val_loss: 0.2551 - val_accuracy: 0.0514\n",
      "Epoch 117/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2121 - accuracy: 0.2254\n",
      "Epoch 117: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2122 - accuracy: 0.2255 - val_loss: 0.2556 - val_accuracy: 0.0578\n",
      "Epoch 118/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2109 - accuracy: 0.2273\n",
      "Epoch 118: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.2109 - accuracy: 0.2271 - val_loss: 0.2554 - val_accuracy: 0.0621\n",
      "Epoch 119/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2097 - accuracy: 0.2238\n",
      "Epoch 119: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2097 - accuracy: 0.2238 - val_loss: 0.2554 - val_accuracy: 0.0535\n",
      "Epoch 120/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2086 - accuracy: 0.2399\n",
      "Epoch 120: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2086 - accuracy: 0.2397 - val_loss: 0.2566 - val_accuracy: 0.0621\n",
      "Epoch 121/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2088 - accuracy: 0.2279\n",
      "Epoch 121: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2086 - accuracy: 0.2299 - val_loss: 0.2584 - val_accuracy: 0.0514\n",
      "Epoch 122/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2079 - accuracy: 0.2315\n",
      "Epoch 122: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2077 - accuracy: 0.2328 - val_loss: 0.2580 - val_accuracy: 0.0450\n",
      "Epoch 123/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2071 - accuracy: 0.2333\n",
      "Epoch 123: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2070 - accuracy: 0.2352 - val_loss: 0.2582 - val_accuracy: 0.0471\n",
      "Epoch 124/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2070 - accuracy: 0.2459\n",
      "Epoch 124: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2070 - accuracy: 0.2461 - val_loss: 0.2582 - val_accuracy: 0.0642\n",
      "Epoch 125/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2059 - accuracy: 0.2449\n",
      "Epoch 125: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2059 - accuracy: 0.2449 - val_loss: 0.2594 - val_accuracy: 0.0578\n",
      "Epoch 126/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2051 - accuracy: 0.2506\n",
      "Epoch 126: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.2051 - accuracy: 0.2506 - val_loss: 0.2599 - val_accuracy: 0.0514\n",
      "Epoch 127/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2047 - accuracy: 0.2496\n",
      "Epoch 127: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.2049 - accuracy: 0.2490 - val_loss: 0.2598 - val_accuracy: 0.0600\n",
      "Epoch 128/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.2474\n",
      "Epoch 128: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2053 - accuracy: 0.2474 - val_loss: 0.2588 - val_accuracy: 0.0578\n",
      "Epoch 129/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2052 - accuracy: 0.2492\n",
      "Epoch 129: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2052 - accuracy: 0.2494 - val_loss: 0.2612 - val_accuracy: 0.0621\n",
      "Epoch 130/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2047 - accuracy: 0.2512\n",
      "Epoch 130: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2048 - accuracy: 0.2518 - val_loss: 0.2589 - val_accuracy: 0.0664\n",
      "Epoch 131/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.2035 - accuracy: 0.2508\n",
      "Epoch 131: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2034 - accuracy: 0.2522 - val_loss: 0.2593 - val_accuracy: 0.0664\n",
      "Epoch 132/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2023 - accuracy: 0.2610\n",
      "Epoch 132: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2023 - accuracy: 0.2612 - val_loss: 0.2618 - val_accuracy: 0.0664\n",
      "Epoch 133/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2019 - accuracy: 0.2570\n",
      "Epoch 133: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2019 - accuracy: 0.2571 - val_loss: 0.2615 - val_accuracy: 0.0621\n",
      "Epoch 134/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.2018 - accuracy: 0.2558\n",
      "Epoch 134: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2018 - accuracy: 0.2559 - val_loss: 0.2625 - val_accuracy: 0.0578\n",
      "Epoch 135/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.2624\n",
      "Epoch 135: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.2031 - accuracy: 0.2624 - val_loss: 0.2619 - val_accuracy: 0.0535\n",
      "Epoch 136/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.2010 - accuracy: 0.2642\n",
      "Epoch 136: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.2010 - accuracy: 0.2644 - val_loss: 0.2636 - val_accuracy: 0.0557\n",
      "Epoch 137/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1999 - accuracy: 0.2729\n",
      "Epoch 137: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.1999 - accuracy: 0.2729 - val_loss: 0.2627 - val_accuracy: 0.0707\n",
      "Epoch 138/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1989 - accuracy: 0.2764\n",
      "Epoch 138: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1989 - accuracy: 0.2770 - val_loss: 0.2665 - val_accuracy: 0.0642\n",
      "Epoch 139/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1985 - accuracy: 0.2677\n",
      "Epoch 139: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1985 - accuracy: 0.2693 - val_loss: 0.2667 - val_accuracy: 0.0621\n",
      "Epoch 140/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1991 - accuracy: 0.2754\n",
      "Epoch 140: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.1990 - accuracy: 0.2766 - val_loss: 0.2675 - val_accuracy: 0.0535\n",
      "Epoch 141/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1975 - accuracy: 0.2825\n",
      "Epoch 141: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1975 - accuracy: 0.2822 - val_loss: 0.2662 - val_accuracy: 0.0642\n",
      "Epoch 142/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1958 - accuracy: 0.2858\n",
      "Epoch 142: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1959 - accuracy: 0.2851 - val_loss: 0.2682 - val_accuracy: 0.0642\n",
      "Epoch 143/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1968 - accuracy: 0.2887\n",
      "Epoch 143: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1968 - accuracy: 0.2887 - val_loss: 0.2681 - val_accuracy: 0.0600\n",
      "Epoch 144/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1968 - accuracy: 0.2841\n",
      "Epoch 144: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1968 - accuracy: 0.2822 - val_loss: 0.2689 - val_accuracy: 0.0664\n",
      "Epoch 145/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1959 - accuracy: 0.2946\n",
      "Epoch 145: val_accuracy did not improve from 0.07281\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1958 - accuracy: 0.2960 - val_loss: 0.2694 - val_accuracy: 0.0578\n",
      "Epoch 146/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1941 - accuracy: 0.2992\n",
      "Epoch 146: val_accuracy improved from 0.07281 to 0.07923, saving model to ./model\\20231204-192802B_LSTM2_batchsize.h5\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1942 - accuracy: 0.2985 - val_loss: 0.2696 - val_accuracy: 0.0792\n",
      "Epoch 147/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1946 - accuracy: 0.2926\n",
      "Epoch 147: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1945 - accuracy: 0.2924 - val_loss: 0.2704 - val_accuracy: 0.0685\n",
      "Epoch 148/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1923 - accuracy: 0.3113\n",
      "Epoch 148: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1923 - accuracy: 0.3114 - val_loss: 0.2720 - val_accuracy: 0.0621\n",
      "Epoch 149/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1919 - accuracy: 0.3033\n",
      "Epoch 149: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1918 - accuracy: 0.3078 - val_loss: 0.2731 - val_accuracy: 0.0685\n",
      "Epoch 150/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1937 - accuracy: 0.3004\n",
      "Epoch 150: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1935 - accuracy: 0.3021 - val_loss: 0.2722 - val_accuracy: 0.0535\n",
      "Epoch 151/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.3183\n",
      "Epoch 151: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1909 - accuracy: 0.3183 - val_loss: 0.2743 - val_accuracy: 0.0600\n",
      "Epoch 152/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1900 - accuracy: 0.3199\n",
      "Epoch 152: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1900 - accuracy: 0.3204 - val_loss: 0.2760 - val_accuracy: 0.0557\n",
      "Epoch 153/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1893 - accuracy: 0.3222\n",
      "Epoch 153: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1893 - accuracy: 0.3224 - val_loss: 0.2763 - val_accuracy: 0.0664\n",
      "Epoch 154/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1882 - accuracy: 0.3309\n",
      "Epoch 154: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1882 - accuracy: 0.3309 - val_loss: 0.2753 - val_accuracy: 0.0578\n",
      "Epoch 155/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1878 - accuracy: 0.3289\n",
      "Epoch 155: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.1878 - accuracy: 0.3289 - val_loss: 0.2783 - val_accuracy: 0.0685\n",
      "Epoch 156/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1879 - accuracy: 0.3250\n",
      "Epoch 156: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1878 - accuracy: 0.3244 - val_loss: 0.2786 - val_accuracy: 0.0600\n",
      "Epoch 157/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1877 - accuracy: 0.3335\n",
      "Epoch 157: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1877 - accuracy: 0.3341 - val_loss: 0.2794 - val_accuracy: 0.0642\n",
      "Epoch 158/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1870 - accuracy: 0.3386\n",
      "Epoch 158: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1870 - accuracy: 0.3386 - val_loss: 0.2795 - val_accuracy: 0.0557\n",
      "Epoch 159/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1855 - accuracy: 0.3374\n",
      "Epoch 159: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.1855 - accuracy: 0.3374 - val_loss: 0.2815 - val_accuracy: 0.0493\n",
      "Epoch 160/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1858 - accuracy: 0.3340\n",
      "Epoch 160: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1858 - accuracy: 0.3337 - val_loss: 0.2810 - val_accuracy: 0.0493\n",
      "Epoch 161/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1877 - accuracy: 0.3379\n",
      "Epoch 161: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1875 - accuracy: 0.3370 - val_loss: 0.2802 - val_accuracy: 0.0428\n",
      "Epoch 162/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1830 - accuracy: 0.3458\n",
      "Epoch 162: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.1829 - accuracy: 0.3471 - val_loss: 0.2828 - val_accuracy: 0.0471\n",
      "Epoch 163/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1816 - accuracy: 0.3679\n",
      "Epoch 163: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1816 - accuracy: 0.3694 - val_loss: 0.2830 - val_accuracy: 0.0642\n",
      "Epoch 164/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1812 - accuracy: 0.3680\n",
      "Epoch 164: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.1810 - accuracy: 0.3694 - val_loss: 0.2858 - val_accuracy: 0.0450\n",
      "Epoch 165/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1804 - accuracy: 0.3666\n",
      "Epoch 165: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1804 - accuracy: 0.3666 - val_loss: 0.2853 - val_accuracy: 0.0535\n",
      "Epoch 166/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1790 - accuracy: 0.3873\n",
      "Epoch 166: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1790 - accuracy: 0.3873 - val_loss: 0.2852 - val_accuracy: 0.0471\n",
      "Epoch 167/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1792 - accuracy: 0.3734\n",
      "Epoch 167: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1793 - accuracy: 0.3735 - val_loss: 0.2869 - val_accuracy: 0.0407\n",
      "Epoch 168/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1779 - accuracy: 0.3791\n",
      "Epoch 168: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1779 - accuracy: 0.3800 - val_loss: 0.2879 - val_accuracy: 0.0535\n",
      "Epoch 169/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1776 - accuracy: 0.3782\n",
      "Epoch 169: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1776 - accuracy: 0.3779 - val_loss: 0.2885 - val_accuracy: 0.0471\n",
      "Epoch 170/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1772 - accuracy: 0.3846\n",
      "Epoch 170: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1772 - accuracy: 0.3852 - val_loss: 0.2914 - val_accuracy: 0.0578\n",
      "Epoch 171/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1758 - accuracy: 0.3945\n",
      "Epoch 171: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1758 - accuracy: 0.3946 - val_loss: 0.2913 - val_accuracy: 0.0535\n",
      "Epoch 172/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1753 - accuracy: 0.3905\n",
      "Epoch 172: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1753 - accuracy: 0.3905 - val_loss: 0.2944 - val_accuracy: 0.0514\n",
      "Epoch 173/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1745 - accuracy: 0.3916\n",
      "Epoch 173: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.1745 - accuracy: 0.3913 - val_loss: 0.2937 - val_accuracy: 0.0493\n",
      "Epoch 174/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1750 - accuracy: 0.4000\n",
      "Epoch 174: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1750 - accuracy: 0.3990 - val_loss: 0.2944 - val_accuracy: 0.0535\n",
      "Epoch 175/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1742 - accuracy: 0.4171\n",
      "Epoch 175: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1738 - accuracy: 0.4185 - val_loss: 0.2959 - val_accuracy: 0.0578\n",
      "Epoch 176/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.4075\n",
      "Epoch 176: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1729 - accuracy: 0.4075 - val_loss: 0.2962 - val_accuracy: 0.0578\n",
      "Epoch 177/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1712 - accuracy: 0.4235\n",
      "Epoch 177: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1715 - accuracy: 0.4213 - val_loss: 0.2968 - val_accuracy: 0.0493\n",
      "Epoch 178/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1713 - accuracy: 0.4250\n",
      "Epoch 178: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1713 - accuracy: 0.4250 - val_loss: 0.2997 - val_accuracy: 0.0642\n",
      "Epoch 179/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1737 - accuracy: 0.4079\n",
      "Epoch 179: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 0.1737 - accuracy: 0.4075 - val_loss: 0.2963 - val_accuracy: 0.0514\n",
      "Epoch 180/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1689 - accuracy: 0.4305\n",
      "Epoch 180: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.1689 - accuracy: 0.4303 - val_loss: 0.2982 - val_accuracy: 0.0578\n",
      "Epoch 181/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1685 - accuracy: 0.4442\n",
      "Epoch 181: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1684 - accuracy: 0.4448 - val_loss: 0.3011 - val_accuracy: 0.0621\n",
      "Epoch 182/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1665 - accuracy: 0.4383\n",
      "Epoch 182: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1665 - accuracy: 0.4384 - val_loss: 0.3032 - val_accuracy: 0.0578\n",
      "Epoch 183/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1652 - accuracy: 0.4598\n",
      "Epoch 183: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1652 - accuracy: 0.4594 - val_loss: 0.3036 - val_accuracy: 0.0450\n",
      "Epoch 184/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1650 - accuracy: 0.4560\n",
      "Epoch 184: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1647 - accuracy: 0.4578 - val_loss: 0.3068 - val_accuracy: 0.0514\n",
      "Epoch 185/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1653 - accuracy: 0.4521\n",
      "Epoch 185: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1652 - accuracy: 0.4534 - val_loss: 0.3068 - val_accuracy: 0.0471\n",
      "Epoch 186/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1644 - accuracy: 0.4574\n",
      "Epoch 186: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1644 - accuracy: 0.4578 - val_loss: 0.3082 - val_accuracy: 0.0578\n",
      "Epoch 187/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1624 - accuracy: 0.4683\n",
      "Epoch 187: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1625 - accuracy: 0.4680 - val_loss: 0.3072 - val_accuracy: 0.0600\n",
      "Epoch 188/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1615 - accuracy: 0.4725\n",
      "Epoch 188: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1614 - accuracy: 0.4753 - val_loss: 0.3159 - val_accuracy: 0.0493\n",
      "Epoch 189/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1624 - accuracy: 0.4614\n",
      "Epoch 189: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.1624 - accuracy: 0.4615 - val_loss: 0.3108 - val_accuracy: 0.0557\n",
      "Epoch 190/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1604 - accuracy: 0.4830\n",
      "Epoch 190: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1604 - accuracy: 0.4826 - val_loss: 0.3146 - val_accuracy: 0.0493\n",
      "Epoch 191/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1599 - accuracy: 0.4793\n",
      "Epoch 191: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1599 - accuracy: 0.4793 - val_loss: 0.3152 - val_accuracy: 0.0514\n",
      "Epoch 192/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1591 - accuracy: 0.4850\n",
      "Epoch 192: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1592 - accuracy: 0.4854 - val_loss: 0.3158 - val_accuracy: 0.0493\n",
      "Epoch 193/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1588 - accuracy: 0.4811\n",
      "Epoch 193: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1587 - accuracy: 0.4822 - val_loss: 0.3151 - val_accuracy: 0.0578\n",
      "Epoch 194/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1572 - accuracy: 0.5004\n",
      "Epoch 194: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1572 - accuracy: 0.5004 - val_loss: 0.3198 - val_accuracy: 0.0621\n",
      "Epoch 195/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1567 - accuracy: 0.4955\n",
      "Epoch 195: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1566 - accuracy: 0.4980 - val_loss: 0.3181 - val_accuracy: 0.0557\n",
      "Epoch 196/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1546 - accuracy: 0.5070\n",
      "Epoch 196: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1548 - accuracy: 0.5065 - val_loss: 0.3215 - val_accuracy: 0.0557\n",
      "Epoch 197/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1537 - accuracy: 0.5110\n",
      "Epoch 197: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1537 - accuracy: 0.5105 - val_loss: 0.3231 - val_accuracy: 0.0621\n",
      "Epoch 198/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1531 - accuracy: 0.5152\n",
      "Epoch 198: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1531 - accuracy: 0.5146 - val_loss: 0.3224 - val_accuracy: 0.0535\n",
      "Epoch 199/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.5062\n",
      "Epoch 199: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1545 - accuracy: 0.5081 - val_loss: 0.3247 - val_accuracy: 0.0557\n",
      "Epoch 200/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1538 - accuracy: 0.5105\n",
      "Epoch 200: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.1538 - accuracy: 0.5105 - val_loss: 0.3243 - val_accuracy: 0.0600\n",
      "Epoch 201/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1529 - accuracy: 0.5154\n",
      "Epoch 201: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.1528 - accuracy: 0.5158 - val_loss: 0.3278 - val_accuracy: 0.0493\n",
      "Epoch 202/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1504 - accuracy: 0.5264\n",
      "Epoch 202: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.1504 - accuracy: 0.5264 - val_loss: 0.3287 - val_accuracy: 0.0600\n",
      "Epoch 203/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1497 - accuracy: 0.5250\n",
      "Epoch 203: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1493 - accuracy: 0.5264 - val_loss: 0.3302 - val_accuracy: 0.0557\n",
      "Epoch 204/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1482 - accuracy: 0.5365\n",
      "Epoch 204: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.1482 - accuracy: 0.5365 - val_loss: 0.3327 - val_accuracy: 0.0493\n",
      "Epoch 205/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1470 - accuracy: 0.5389\n",
      "Epoch 205: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1470 - accuracy: 0.5389 - val_loss: 0.3332 - val_accuracy: 0.0493\n",
      "Epoch 206/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1486 - accuracy: 0.5426\n",
      "Epoch 206: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.1486 - accuracy: 0.5426 - val_loss: 0.3342 - val_accuracy: 0.0578\n",
      "Epoch 207/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1473 - accuracy: 0.5354\n",
      "Epoch 207: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1475 - accuracy: 0.5365 - val_loss: 0.3359 - val_accuracy: 0.0557\n",
      "Epoch 208/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1462 - accuracy: 0.5462\n",
      "Epoch 208: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1462 - accuracy: 0.5462 - val_loss: 0.3377 - val_accuracy: 0.0578\n",
      "Epoch 209/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1451 - accuracy: 0.5608\n",
      "Epoch 209: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.1454 - accuracy: 0.5584 - val_loss: 0.3452 - val_accuracy: 0.0471\n",
      "Epoch 210/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1440 - accuracy: 0.5492\n",
      "Epoch 210: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1440 - accuracy: 0.5507 - val_loss: 0.3374 - val_accuracy: 0.0621\n",
      "Epoch 211/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1446 - accuracy: 0.5508\n",
      "Epoch 211: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1445 - accuracy: 0.5511 - val_loss: 0.3386 - val_accuracy: 0.0557\n",
      "Epoch 212/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1432 - accuracy: 0.5535\n",
      "Epoch 212: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.1432 - accuracy: 0.5535 - val_loss: 0.3412 - val_accuracy: 0.0578\n",
      "Epoch 213/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1400 - accuracy: 0.5836\n",
      "Epoch 213: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.1400 - accuracy: 0.5835 - val_loss: 0.3412 - val_accuracy: 0.0685\n",
      "Epoch 214/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1407 - accuracy: 0.5744\n",
      "Epoch 214: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1407 - accuracy: 0.5738 - val_loss: 0.3446 - val_accuracy: 0.0514\n",
      "Epoch 215/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1381 - accuracy: 0.5822\n",
      "Epoch 215: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.1380 - accuracy: 0.5835 - val_loss: 0.3484 - val_accuracy: 0.0535\n",
      "Epoch 216/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1370 - accuracy: 0.5881\n",
      "Epoch 216: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.1370 - accuracy: 0.5884 - val_loss: 0.3473 - val_accuracy: 0.0600\n",
      "Epoch 217/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1366 - accuracy: 0.5900\n",
      "Epoch 217: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.1363 - accuracy: 0.5916 - val_loss: 0.3482 - val_accuracy: 0.0578\n",
      "Epoch 218/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.6010\n",
      "Epoch 218: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.1354 - accuracy: 0.6010 - val_loss: 0.3461 - val_accuracy: 0.0514\n",
      "Epoch 219/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1349 - accuracy: 0.5961\n",
      "Epoch 219: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 24ms/step - loss: 0.1349 - accuracy: 0.5961 - val_loss: 0.3563 - val_accuracy: 0.0450\n",
      "Epoch 220/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1346 - accuracy: 0.5933\n",
      "Epoch 220: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.1346 - accuracy: 0.5933 - val_loss: 0.3536 - val_accuracy: 0.0728\n",
      "Epoch 221/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1348 - accuracy: 0.5933\n",
      "Epoch 221: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.1349 - accuracy: 0.5933 - val_loss: 0.3577 - val_accuracy: 0.0471\n",
      "Epoch 222/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1339 - accuracy: 0.6025\n",
      "Epoch 222: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1336 - accuracy: 0.6026 - val_loss: 0.3562 - val_accuracy: 0.0557\n",
      "Epoch 223/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1326 - accuracy: 0.6058\n",
      "Epoch 223: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.1326 - accuracy: 0.6058 - val_loss: 0.3614 - val_accuracy: 0.0557\n",
      "Epoch 224/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1321 - accuracy: 0.6135\n",
      "Epoch 224: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.1321 - accuracy: 0.6135 - val_loss: 0.3626 - val_accuracy: 0.0535\n",
      "Epoch 225/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1298 - accuracy: 0.6160\n",
      "Epoch 225: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1297 - accuracy: 0.6160 - val_loss: 0.3653 - val_accuracy: 0.0535\n",
      "Epoch 226/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1340 - accuracy: 0.6028\n",
      "Epoch 226: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.1337 - accuracy: 0.6042 - val_loss: 0.3654 - val_accuracy: 0.0535\n",
      "Epoch 227/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1308 - accuracy: 0.6192\n",
      "Epoch 227: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1304 - accuracy: 0.6221 - val_loss: 0.3670 - val_accuracy: 0.0578\n",
      "Epoch 228/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1298 - accuracy: 0.6115\n",
      "Epoch 228: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1298 - accuracy: 0.6115 - val_loss: 0.3661 - val_accuracy: 0.0664\n",
      "Epoch 229/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.6250\n",
      "Epoch 229: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1277 - accuracy: 0.6241 - val_loss: 0.3677 - val_accuracy: 0.0685\n",
      "Epoch 230/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1263 - accuracy: 0.6375\n",
      "Epoch 230: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1262 - accuracy: 0.6363 - val_loss: 0.3704 - val_accuracy: 0.0471\n",
      "Epoch 231/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1244 - accuracy: 0.6450\n",
      "Epoch 231: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1244 - accuracy: 0.6440 - val_loss: 0.3749 - val_accuracy: 0.0557\n",
      "Epoch 232/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1229 - accuracy: 0.6579\n",
      "Epoch 232: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1226 - accuracy: 0.6602 - val_loss: 0.3718 - val_accuracy: 0.0642\n",
      "Epoch 233/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1230 - accuracy: 0.6557\n",
      "Epoch 233: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1230 - accuracy: 0.6557 - val_loss: 0.3741 - val_accuracy: 0.0557\n",
      "Epoch 234/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1229 - accuracy: 0.6488\n",
      "Epoch 234: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1224 - accuracy: 0.6500 - val_loss: 0.3730 - val_accuracy: 0.0621\n",
      "Epoch 235/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.6648\n",
      "Epoch 235: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1210 - accuracy: 0.6646 - val_loss: 0.3761 - val_accuracy: 0.0664\n",
      "Epoch 236/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.6583\n",
      "Epoch 236: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1212 - accuracy: 0.6586 - val_loss: 0.3781 - val_accuracy: 0.0600\n",
      "Epoch 237/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1195 - accuracy: 0.6725\n",
      "Epoch 237: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.1196 - accuracy: 0.6719 - val_loss: 0.3822 - val_accuracy: 0.0771\n",
      "Epoch 238/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1212 - accuracy: 0.6633\n",
      "Epoch 238: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1210 - accuracy: 0.6626 - val_loss: 0.3813 - val_accuracy: 0.0664\n",
      "Epoch 239/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1175 - accuracy: 0.6764\n",
      "Epoch 239: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1175 - accuracy: 0.6764 - val_loss: 0.3848 - val_accuracy: 0.0728\n",
      "Epoch 240/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1154 - accuracy: 0.6887\n",
      "Epoch 240: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1151 - accuracy: 0.6902 - val_loss: 0.3895 - val_accuracy: 0.0578\n",
      "Epoch 241/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.7007\n",
      "Epoch 241: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1145 - accuracy: 0.7007 - val_loss: 0.3863 - val_accuracy: 0.0664\n",
      "Epoch 242/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1138 - accuracy: 0.6999\n",
      "Epoch 242: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1138 - accuracy: 0.6999 - val_loss: 0.3904 - val_accuracy: 0.0685\n",
      "Epoch 243/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1142 - accuracy: 0.6956\n",
      "Epoch 243: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1142 - accuracy: 0.6959 - val_loss: 0.3903 - val_accuracy: 0.0642\n",
      "Epoch 244/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1134 - accuracy: 0.6990\n",
      "Epoch 244: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1133 - accuracy: 0.6995 - val_loss: 0.3995 - val_accuracy: 0.0664\n",
      "Epoch 245/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1124 - accuracy: 0.6997\n",
      "Epoch 245: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.1124 - accuracy: 0.6995 - val_loss: 0.3945 - val_accuracy: 0.0664\n",
      "Epoch 246/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1109 - accuracy: 0.7048\n",
      "Epoch 246: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1109 - accuracy: 0.7048 - val_loss: 0.3949 - val_accuracy: 0.0621\n",
      "Epoch 247/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1093 - accuracy: 0.7216\n",
      "Epoch 247: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1093 - accuracy: 0.7210 - val_loss: 0.3990 - val_accuracy: 0.0621\n",
      "Epoch 248/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.7113\n",
      "Epoch 248: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1101 - accuracy: 0.7113 - val_loss: 0.3991 - val_accuracy: 0.0664\n",
      "Epoch 249/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1098 - accuracy: 0.7104\n",
      "Epoch 249: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1096 - accuracy: 0.7121 - val_loss: 0.4007 - val_accuracy: 0.0707\n",
      "Epoch 250/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1074 - accuracy: 0.7278\n",
      "Epoch 250: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.1074 - accuracy: 0.7271 - val_loss: 0.4032 - val_accuracy: 0.0664\n",
      "Epoch 251/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.7109\n",
      "Epoch 251: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1087 - accuracy: 0.7109 - val_loss: 0.4056 - val_accuracy: 0.0771\n",
      "Epoch 252/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1066 - accuracy: 0.7321\n",
      "Epoch 252: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1066 - accuracy: 0.7324 - val_loss: 0.4092 - val_accuracy: 0.0664\n",
      "Epoch 253/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1043 - accuracy: 0.7329\n",
      "Epoch 253: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 0.1047 - accuracy: 0.7311 - val_loss: 0.4117 - val_accuracy: 0.0664\n",
      "Epoch 254/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1043 - accuracy: 0.7346\n",
      "Epoch 254: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1043 - accuracy: 0.7344 - val_loss: 0.4146 - val_accuracy: 0.0600\n",
      "Epoch 255/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1046 - accuracy: 0.7393\n",
      "Epoch 255: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1045 - accuracy: 0.7401 - val_loss: 0.4180 - val_accuracy: 0.0728\n",
      "Epoch 256/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.7482\n",
      "Epoch 256: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1029 - accuracy: 0.7482 - val_loss: 0.4168 - val_accuracy: 0.0771\n",
      "Epoch 257/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1022 - accuracy: 0.7458\n",
      "Epoch 257: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1018 - accuracy: 0.7482 - val_loss: 0.4170 - val_accuracy: 0.0685\n",
      "Epoch 258/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1002 - accuracy: 0.7575\n",
      "Epoch 258: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1002 - accuracy: 0.7575 - val_loss: 0.4212 - val_accuracy: 0.0685\n",
      "Epoch 259/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.7388\n",
      "Epoch 259: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1010 - accuracy: 0.7388 - val_loss: 0.4260 - val_accuracy: 0.0621\n",
      "Epoch 260/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.1009 - accuracy: 0.7551\n",
      "Epoch 260: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.1009 - accuracy: 0.7551 - val_loss: 0.4248 - val_accuracy: 0.0621\n",
      "Epoch 261/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0982 - accuracy: 0.7616\n",
      "Epoch 261: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0982 - accuracy: 0.7616 - val_loss: 0.4299 - val_accuracy: 0.0621\n",
      "Epoch 262/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0973 - accuracy: 0.7592\n",
      "Epoch 262: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0973 - accuracy: 0.7612 - val_loss: 0.4278 - val_accuracy: 0.0664\n",
      "Epoch 263/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0954 - accuracy: 0.7756\n",
      "Epoch 263: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0954 - accuracy: 0.7758 - val_loss: 0.4263 - val_accuracy: 0.0664\n",
      "Epoch 264/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0959 - accuracy: 0.7692\n",
      "Epoch 264: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0960 - accuracy: 0.7689 - val_loss: 0.4327 - val_accuracy: 0.0685\n",
      "Epoch 265/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.7723\n",
      "Epoch 265: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.0960 - accuracy: 0.7725 - val_loss: 0.4387 - val_accuracy: 0.0600\n",
      "Epoch 266/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0952 - accuracy: 0.7658\n",
      "Epoch 266: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0948 - accuracy: 0.7680 - val_loss: 0.4320 - val_accuracy: 0.0557\n",
      "Epoch 267/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.7920\n",
      "Epoch 267: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0922 - accuracy: 0.7920 - val_loss: 0.4360 - val_accuracy: 0.0642\n",
      "Epoch 268/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0908 - accuracy: 0.7894\n",
      "Epoch 268: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 22ms/step - loss: 0.0907 - accuracy: 0.7895 - val_loss: 0.4412 - val_accuracy: 0.0664\n",
      "Epoch 269/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0902 - accuracy: 0.7914\n",
      "Epoch 269: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0901 - accuracy: 0.7916 - val_loss: 0.4405 - val_accuracy: 0.0707\n",
      "Epoch 270/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0886 - accuracy: 0.8030\n",
      "Epoch 270: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0887 - accuracy: 0.8021 - val_loss: 0.4417 - val_accuracy: 0.0578\n",
      "Epoch 271/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0894 - accuracy: 0.7996\n",
      "Epoch 271: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.0890 - accuracy: 0.8021 - val_loss: 0.4450 - val_accuracy: 0.0642\n",
      "Epoch 272/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.8082\n",
      "Epoch 272: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0887 - accuracy: 0.8082 - val_loss: 0.4478 - val_accuracy: 0.0685\n",
      "Epoch 273/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.8055\n",
      "Epoch 273: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0883 - accuracy: 0.8049 - val_loss: 0.4451 - val_accuracy: 0.0642\n",
      "Epoch 274/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0873 - accuracy: 0.8054\n",
      "Epoch 274: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0873 - accuracy: 0.8058 - val_loss: 0.4541 - val_accuracy: 0.0642\n",
      "Epoch 275/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0867 - accuracy: 0.8092\n",
      "Epoch 275: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0866 - accuracy: 0.8106 - val_loss: 0.4518 - val_accuracy: 0.0578\n",
      "Epoch 276/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0845 - accuracy: 0.8141\n",
      "Epoch 276: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0846 - accuracy: 0.8139 - val_loss: 0.4582 - val_accuracy: 0.0557\n",
      "Epoch 277/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0856 - accuracy: 0.8100\n",
      "Epoch 277: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.0855 - accuracy: 0.8102 - val_loss: 0.4586 - val_accuracy: 0.0664\n",
      "Epoch 278/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0869 - accuracy: 0.7998\n",
      "Epoch 278: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0871 - accuracy: 0.8001 - val_loss: 0.4584 - val_accuracy: 0.0685\n",
      "Epoch 279/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.8240\n",
      "Epoch 279: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0832 - accuracy: 0.8240 - val_loss: 0.4587 - val_accuracy: 0.0578\n",
      "Epoch 280/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.8279\n",
      "Epoch 280: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0826 - accuracy: 0.8281 - val_loss: 0.4621 - val_accuracy: 0.0535\n",
      "Epoch 281/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0814 - accuracy: 0.8348\n",
      "Epoch 281: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 22ms/step - loss: 0.0813 - accuracy: 0.8350 - val_loss: 0.4675 - val_accuracy: 0.0621\n",
      "Epoch 282/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.8352\n",
      "Epoch 282: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0807 - accuracy: 0.8354 - val_loss: 0.4687 - val_accuracy: 0.0578\n",
      "Epoch 283/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0795 - accuracy: 0.8417\n",
      "Epoch 283: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0792 - accuracy: 0.8427 - val_loss: 0.4672 - val_accuracy: 0.0685\n",
      "Epoch 284/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0867 - accuracy: 0.7996\n",
      "Epoch 284: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0868 - accuracy: 0.8001 - val_loss: 0.4738 - val_accuracy: 0.0514\n",
      "Epoch 285/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0862 - accuracy: 0.8050\n",
      "Epoch 285: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0861 - accuracy: 0.8058 - val_loss: 0.4753 - val_accuracy: 0.0621\n",
      "Epoch 286/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0846 - accuracy: 0.8096\n",
      "Epoch 286: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0844 - accuracy: 0.8102 - val_loss: 0.4777 - val_accuracy: 0.0664\n",
      "Epoch 287/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.8503\n",
      "Epoch 287: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0782 - accuracy: 0.8516 - val_loss: 0.4784 - val_accuracy: 0.0621\n",
      "Epoch 288/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.8474\n",
      "Epoch 288: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0774 - accuracy: 0.8475 - val_loss: 0.4823 - val_accuracy: 0.0600\n",
      "Epoch 289/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0754 - accuracy: 0.8575\n",
      "Epoch 289: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0752 - accuracy: 0.8581 - val_loss: 0.4809 - val_accuracy: 0.0621\n",
      "Epoch 290/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.8590\n",
      "Epoch 290: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0760 - accuracy: 0.8593 - val_loss: 0.4791 - val_accuracy: 0.0664\n",
      "Epoch 291/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.8605\n",
      "Epoch 291: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0746 - accuracy: 0.8605 - val_loss: 0.4841 - val_accuracy: 0.0642\n",
      "Epoch 292/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.8601\n",
      "Epoch 292: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 16ms/step - loss: 0.0735 - accuracy: 0.8601 - val_loss: 0.4902 - val_accuracy: 0.0642\n",
      "Epoch 293/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0728 - accuracy: 0.8700\n",
      "Epoch 293: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0726 - accuracy: 0.8710 - val_loss: 0.4901 - val_accuracy: 0.0664\n",
      "Epoch 294/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.8398\n",
      "Epoch 294: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 0.0777 - accuracy: 0.8398 - val_loss: 0.4961 - val_accuracy: 0.0664\n",
      "Epoch 295/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.8273\n",
      "Epoch 295: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 0.0807 - accuracy: 0.8273 - val_loss: 0.4879 - val_accuracy: 0.0642\n",
      "Epoch 296/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0769 - accuracy: 0.8446\n",
      "Epoch 296: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.0768 - accuracy: 0.8451 - val_loss: 0.4969 - val_accuracy: 0.0600\n",
      "Epoch 297/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.8698\n",
      "Epoch 297: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.0727 - accuracy: 0.8698 - val_loss: 0.4932 - val_accuracy: 0.0664\n",
      "Epoch 298/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0700 - accuracy: 0.8742\n",
      "Epoch 298: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.0698 - accuracy: 0.8747 - val_loss: 0.4968 - val_accuracy: 0.0642\n",
      "Epoch 299/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0672 - accuracy: 0.8916\n",
      "Epoch 299: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 0.0672 - accuracy: 0.8917 - val_loss: 0.5017 - val_accuracy: 0.0600\n",
      "Epoch 300/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.8994\n",
      "Epoch 300: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.0662 - accuracy: 0.8994 - val_loss: 0.5083 - val_accuracy: 0.0642\n",
      "Epoch 301/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0668 - accuracy: 0.8840\n",
      "Epoch 301: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0668 - accuracy: 0.8840 - val_loss: 0.5100 - val_accuracy: 0.0771\n",
      "Epoch 302/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0656 - accuracy: 0.8960\n",
      "Epoch 302: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0656 - accuracy: 0.8962 - val_loss: 0.5095 - val_accuracy: 0.0685\n",
      "Epoch 303/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9011\n",
      "Epoch 303: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.0639 - accuracy: 0.9011 - val_loss: 0.5136 - val_accuracy: 0.0664\n",
      "Epoch 304/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0643 - accuracy: 0.8943\n",
      "Epoch 304: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 25ms/step - loss: 0.0645 - accuracy: 0.8933 - val_loss: 0.5186 - val_accuracy: 0.0707\n",
      "Epoch 305/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.8988\n",
      "Epoch 305: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0637 - accuracy: 0.8990 - val_loss: 0.5225 - val_accuracy: 0.0728\n",
      "Epoch 306/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0644 - accuracy: 0.9006\n",
      "Epoch 306: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0644 - accuracy: 0.9006 - val_loss: 0.5143 - val_accuracy: 0.0600\n",
      "Epoch 307/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0643 - accuracy: 0.8937\n",
      "Epoch 307: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0643 - accuracy: 0.8938 - val_loss: 0.5197 - val_accuracy: 0.0685\n",
      "Epoch 308/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0652 - accuracy: 0.8931\n",
      "Epoch 308: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0653 - accuracy: 0.8933 - val_loss: 0.5234 - val_accuracy: 0.0642\n",
      "Epoch 309/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9075\n",
      "Epoch 309: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0621 - accuracy: 0.9075 - val_loss: 0.5300 - val_accuracy: 0.0685\n",
      "Epoch 310/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9017\n",
      "Epoch 310: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0613 - accuracy: 0.9019 - val_loss: 0.5308 - val_accuracy: 0.0642\n",
      "Epoch 311/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0599 - accuracy: 0.9107\n",
      "Epoch 311: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0599 - accuracy: 0.9108 - val_loss: 0.5306 - val_accuracy: 0.0728\n",
      "Epoch 312/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0600 - accuracy: 0.9127\n",
      "Epoch 312: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0601 - accuracy: 0.9124 - val_loss: 0.5420 - val_accuracy: 0.0685\n",
      "Epoch 313/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.8882\n",
      "Epoch 313: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0642 - accuracy: 0.8897 - val_loss: 0.5385 - val_accuracy: 0.0771\n",
      "Epoch 314/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0607 - accuracy: 0.8988\n",
      "Epoch 314: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.0608 - accuracy: 0.8990 - val_loss: 0.5370 - val_accuracy: 0.0685\n",
      "Epoch 315/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0592 - accuracy: 0.9067\n",
      "Epoch 315: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 0.0592 - accuracy: 0.9067 - val_loss: 0.5399 - val_accuracy: 0.0707\n",
      "Epoch 316/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9157\n",
      "Epoch 316: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0576 - accuracy: 0.9157 - val_loss: 0.5442 - val_accuracy: 0.0728\n",
      "Epoch 317/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0561 - accuracy: 0.9213\n",
      "Epoch 317: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0560 - accuracy: 0.9213 - val_loss: 0.5446 - val_accuracy: 0.0728\n",
      "Epoch 318/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0532 - accuracy: 0.9354\n",
      "Epoch 318: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0533 - accuracy: 0.9359 - val_loss: 0.5460 - val_accuracy: 0.0664\n",
      "Epoch 319/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0528 - accuracy: 0.9371\n",
      "Epoch 319: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0528 - accuracy: 0.9371 - val_loss: 0.5508 - val_accuracy: 0.0621\n",
      "Epoch 320/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0541 - accuracy: 0.9330\n",
      "Epoch 320: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0541 - accuracy: 0.9335 - val_loss: 0.5548 - val_accuracy: 0.0642\n",
      "Epoch 321/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0549 - accuracy: 0.9248\n",
      "Epoch 321: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0549 - accuracy: 0.9250 - val_loss: 0.5595 - val_accuracy: 0.0578\n",
      "Epoch 322/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0561 - accuracy: 0.9209\n",
      "Epoch 322: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0561 - accuracy: 0.9209 - val_loss: 0.5624 - val_accuracy: 0.0664\n",
      "Epoch 323/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9311\n",
      "Epoch 323: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 24ms/step - loss: 0.0538 - accuracy: 0.9311 - val_loss: 0.5623 - val_accuracy: 0.0621\n",
      "Epoch 324/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9343\n",
      "Epoch 324: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 27ms/step - loss: 0.0519 - accuracy: 0.9343 - val_loss: 0.5658 - val_accuracy: 0.0685\n",
      "Epoch 325/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0519 - accuracy: 0.9338\n",
      "Epoch 325: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0519 - accuracy: 0.9339 - val_loss: 0.5663 - val_accuracy: 0.0707\n",
      "Epoch 326/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0514 - accuracy: 0.9359\n",
      "Epoch 326: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0514 - accuracy: 0.9351 - val_loss: 0.5658 - val_accuracy: 0.0621\n",
      "Epoch 327/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0499 - accuracy: 0.9464\n",
      "Epoch 327: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.0499 - accuracy: 0.9465 - val_loss: 0.5778 - val_accuracy: 0.0642\n",
      "Epoch 328/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9420\n",
      "Epoch 328: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0502 - accuracy: 0.9420 - val_loss: 0.5746 - val_accuracy: 0.0664\n",
      "Epoch 329/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9424\n",
      "Epoch 329: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 22ms/step - loss: 0.0495 - accuracy: 0.9424 - val_loss: 0.5787 - val_accuracy: 0.0621\n",
      "Epoch 330/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0494 - accuracy: 0.9421\n",
      "Epoch 330: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0494 - accuracy: 0.9424 - val_loss: 0.5835 - val_accuracy: 0.0621\n",
      "Epoch 331/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0507 - accuracy: 0.9371\n",
      "Epoch 331: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0507 - accuracy: 0.9371 - val_loss: 0.5839 - val_accuracy: 0.0664\n",
      "Epoch 332/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0501 - accuracy: 0.9379\n",
      "Epoch 332: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 28ms/step - loss: 0.0501 - accuracy: 0.9380 - val_loss: 0.5862 - val_accuracy: 0.0685\n",
      "Epoch 333/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0505 - accuracy: 0.9383\n",
      "Epoch 333: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 24ms/step - loss: 0.0505 - accuracy: 0.9384 - val_loss: 0.5859 - val_accuracy: 0.0685\n",
      "Epoch 334/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0482 - accuracy: 0.9454\n",
      "Epoch 334: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0484 - accuracy: 0.9440 - val_loss: 0.5866 - val_accuracy: 0.0621\n",
      "Epoch 335/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9485\n",
      "Epoch 335: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0460 - accuracy: 0.9485 - val_loss: 0.5938 - val_accuracy: 0.0664\n",
      "Epoch 336/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9485\n",
      "Epoch 336: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0461 - accuracy: 0.9485 - val_loss: 0.5953 - val_accuracy: 0.0642\n",
      "Epoch 337/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9408\n",
      "Epoch 337: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0480 - accuracy: 0.9408 - val_loss: 0.6009 - val_accuracy: 0.0621\n",
      "Epoch 338/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0465 - accuracy: 0.9461\n",
      "Epoch 338: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0464 - accuracy: 0.9465 - val_loss: 0.5966 - val_accuracy: 0.0685\n",
      "Epoch 339/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9530\n",
      "Epoch 339: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0455 - accuracy: 0.9530 - val_loss: 0.6052 - val_accuracy: 0.0578\n",
      "Epoch 340/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0556 - accuracy: 0.9034\n",
      "Epoch 340: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0556 - accuracy: 0.9031 - val_loss: 0.6107 - val_accuracy: 0.0642\n",
      "Epoch 341/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0560 - accuracy: 0.9119\n",
      "Epoch 341: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0560 - accuracy: 0.9120 - val_loss: 0.6022 - val_accuracy: 0.0642\n",
      "Epoch 342/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0491 - accuracy: 0.9379\n",
      "Epoch 342: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 0.0491 - accuracy: 0.9380 - val_loss: 0.6053 - val_accuracy: 0.0685\n",
      "Epoch 343/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0448 - accuracy: 0.9531\n",
      "Epoch 343: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0448 - accuracy: 0.9534 - val_loss: 0.6006 - val_accuracy: 0.0600\n",
      "Epoch 344/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0408 - accuracy: 0.9638\n",
      "Epoch 344: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 0.0408 - accuracy: 0.9631 - val_loss: 0.6161 - val_accuracy: 0.0621\n",
      "Epoch 345/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0406 - accuracy: 0.9642\n",
      "Epoch 345: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0406 - accuracy: 0.9643 - val_loss: 0.6161 - val_accuracy: 0.0642\n",
      "Epoch 346/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0391 - accuracy: 0.9733\n",
      "Epoch 346: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0390 - accuracy: 0.9728 - val_loss: 0.6157 - val_accuracy: 0.0535\n",
      "Epoch 347/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0381 - accuracy: 0.9729\n",
      "Epoch 347: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0381 - accuracy: 0.9728 - val_loss: 0.6193 - val_accuracy: 0.0600\n",
      "Epoch 348/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9724\n",
      "Epoch 348: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0371 - accuracy: 0.9724 - val_loss: 0.6254 - val_accuracy: 0.0664\n",
      "Epoch 349/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0353 - accuracy: 0.9809\n",
      "Epoch 349: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 22ms/step - loss: 0.0353 - accuracy: 0.9809 - val_loss: 0.6267 - val_accuracy: 0.0642\n",
      "Epoch 350/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0345 - accuracy: 0.9813\n",
      "Epoch 350: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0345 - accuracy: 0.9813 - val_loss: 0.6318 - val_accuracy: 0.0707\n",
      "Epoch 351/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9789\n",
      "Epoch 351: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0346 - accuracy: 0.9789 - val_loss: 0.6342 - val_accuracy: 0.0600\n",
      "Epoch 352/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0343 - accuracy: 0.9794\n",
      "Epoch 352: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 0.0342 - accuracy: 0.9793 - val_loss: 0.6381 - val_accuracy: 0.0578\n",
      "Epoch 353/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0345 - accuracy: 0.9782\n",
      "Epoch 353: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0345 - accuracy: 0.9777 - val_loss: 0.6384 - val_accuracy: 0.0600\n",
      "Epoch 354/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9140\n",
      "Epoch 354: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0511 - accuracy: 0.9140 - val_loss: 0.6389 - val_accuracy: 0.0557\n",
      "Epoch 355/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0453 - accuracy: 0.9403\n",
      "Epoch 355: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0453 - accuracy: 0.9404 - val_loss: 0.6460 - val_accuracy: 0.0514\n",
      "Epoch 356/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0416 - accuracy: 0.9507\n",
      "Epoch 356: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0415 - accuracy: 0.9509 - val_loss: 0.6447 - val_accuracy: 0.0600\n",
      "Epoch 357/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0379 - accuracy: 0.9675\n",
      "Epoch 357: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 0.0379 - accuracy: 0.9676 - val_loss: 0.6523 - val_accuracy: 0.0664\n",
      "Epoch 358/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0359 - accuracy: 0.9721\n",
      "Epoch 358: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 0.0358 - accuracy: 0.9724 - val_loss: 0.6436 - val_accuracy: 0.0621\n",
      "Epoch 359/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0337 - accuracy: 0.9815\n",
      "Epoch 359: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 0.0338 - accuracy: 0.9809 - val_loss: 0.6523 - val_accuracy: 0.0600\n",
      "Epoch 360/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0332 - accuracy: 0.9799\n",
      "Epoch 360: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0333 - accuracy: 0.9801 - val_loss: 0.6517 - val_accuracy: 0.0685\n",
      "Epoch 361/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0324 - accuracy: 0.9781\n",
      "Epoch 361: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0324 - accuracy: 0.9781 - val_loss: 0.6594 - val_accuracy: 0.0642\n",
      "Epoch 362/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0305 - accuracy: 0.9858\n",
      "Epoch 362: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.0305 - accuracy: 0.9854 - val_loss: 0.6587 - val_accuracy: 0.0621\n",
      "Epoch 363/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9874\n",
      "Epoch 363: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0302 - accuracy: 0.9874 - val_loss: 0.6624 - val_accuracy: 0.0621\n",
      "Epoch 364/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0295 - accuracy: 0.9856\n",
      "Epoch 364: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.0296 - accuracy: 0.9854 - val_loss: 0.6617 - val_accuracy: 0.0600\n",
      "Epoch 365/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9854\n",
      "Epoch 365: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 28ms/step - loss: 0.0297 - accuracy: 0.9854 - val_loss: 0.6721 - val_accuracy: 0.0535\n",
      "Epoch 366/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9886\n",
      "Epoch 366: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 28ms/step - loss: 0.0284 - accuracy: 0.9886 - val_loss: 0.6677 - val_accuracy: 0.0685\n",
      "Epoch 367/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0279 - accuracy: 0.9881\n",
      "Epoch 367: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 25ms/step - loss: 0.0279 - accuracy: 0.9878 - val_loss: 0.6710 - val_accuracy: 0.0664\n",
      "Epoch 368/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0285 - accuracy: 0.9862\n",
      "Epoch 368: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0285 - accuracy: 0.9862 - val_loss: 0.6800 - val_accuracy: 0.0578\n",
      "Epoch 369/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0290 - accuracy: 0.9858\n",
      "Epoch 369: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 25ms/step - loss: 0.0290 - accuracy: 0.9858 - val_loss: 0.6732 - val_accuracy: 0.0600\n",
      "Epoch 370/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0286 - accuracy: 0.9873\n",
      "Epoch 370: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 29ms/step - loss: 0.0286 - accuracy: 0.9874 - val_loss: 0.6807 - val_accuracy: 0.0664\n",
      "Epoch 371/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9834\n",
      "Epoch 371: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 3s 33ms/step - loss: 0.0294 - accuracy: 0.9834 - val_loss: 0.6811 - val_accuracy: 0.0578\n",
      "Epoch 372/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0291 - accuracy: 0.9831\n",
      "Epoch 372: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 32ms/step - loss: 0.0290 - accuracy: 0.9834 - val_loss: 0.6799 - val_accuracy: 0.0621\n",
      "Epoch 373/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0302 - accuracy: 0.9809\n",
      "Epoch 373: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 26ms/step - loss: 0.0302 - accuracy: 0.9809 - val_loss: 0.6919 - val_accuracy: 0.0557\n",
      "Epoch 374/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0331 - accuracy: 0.9725\n",
      "Epoch 374: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0329 - accuracy: 0.9732 - val_loss: 0.6878 - val_accuracy: 0.0600\n",
      "Epoch 375/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0321 - accuracy: 0.9745\n",
      "Epoch 375: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.0321 - accuracy: 0.9749 - val_loss: 0.6871 - val_accuracy: 0.0600\n",
      "Epoch 376/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0317 - accuracy: 0.9767\n",
      "Epoch 376: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0316 - accuracy: 0.9765 - val_loss: 0.6867 - val_accuracy: 0.0621\n",
      "Epoch 377/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0306 - accuracy: 0.9778\n",
      "Epoch 377: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 25ms/step - loss: 0.0306 - accuracy: 0.9781 - val_loss: 0.6987 - val_accuracy: 0.0664\n",
      "Epoch 378/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0281 - accuracy: 0.9870\n",
      "Epoch 378: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 0.0280 - accuracy: 0.9870 - val_loss: 0.7034 - val_accuracy: 0.0621\n",
      "Epoch 379/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0271 - accuracy: 0.9867\n",
      "Epoch 379: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0271 - accuracy: 0.9866 - val_loss: 0.6986 - val_accuracy: 0.0664\n",
      "Epoch 380/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0248 - accuracy: 0.9918\n",
      "Epoch 380: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.0248 - accuracy: 0.9915 - val_loss: 0.7055 - val_accuracy: 0.0685\n",
      "Epoch 381/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0240 - accuracy: 0.9911\n",
      "Epoch 381: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0240 - accuracy: 0.9911 - val_loss: 0.7088 - val_accuracy: 0.0621\n",
      "Epoch 382/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0227 - accuracy: 0.9951\n",
      "Epoch 382: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0227 - accuracy: 0.9951 - val_loss: 0.7141 - val_accuracy: 0.0578\n",
      "Epoch 383/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9959\n",
      "Epoch 383: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.0220 - accuracy: 0.9959 - val_loss: 0.7157 - val_accuracy: 0.0600\n",
      "Epoch 384/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9951\n",
      "Epoch 384: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 24ms/step - loss: 0.0214 - accuracy: 0.9951 - val_loss: 0.7153 - val_accuracy: 0.0557\n",
      "Epoch 385/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0207 - accuracy: 0.9951\n",
      "Epoch 385: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 22ms/step - loss: 0.0208 - accuracy: 0.9951 - val_loss: 0.7192 - val_accuracy: 0.0578\n",
      "Epoch 386/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9915\n",
      "Epoch 386: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0227 - accuracy: 0.9915 - val_loss: 0.7238 - val_accuracy: 0.0621\n",
      "Epoch 387/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0329 - accuracy: 0.9700\n",
      "Epoch 387: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 0.0329 - accuracy: 0.9696 - val_loss: 0.7212 - val_accuracy: 0.0621\n",
      "Epoch 388/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0352 - accuracy: 0.9606\n",
      "Epoch 388: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0351 - accuracy: 0.9607 - val_loss: 0.7226 - val_accuracy: 0.0578\n",
      "Epoch 389/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0327 - accuracy: 0.9712\n",
      "Epoch 389: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 0.0327 - accuracy: 0.9712 - val_loss: 0.7328 - val_accuracy: 0.0685\n",
      "Epoch 390/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0281 - accuracy: 0.9797\n",
      "Epoch 390: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0281 - accuracy: 0.9797 - val_loss: 0.7214 - val_accuracy: 0.0664\n",
      "Epoch 391/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9882\n",
      "Epoch 391: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 23ms/step - loss: 0.0260 - accuracy: 0.9882 - val_loss: 0.7270 - val_accuracy: 0.0642\n",
      "Epoch 392/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0216 - accuracy: 0.9942\n",
      "Epoch 392: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 22ms/step - loss: 0.0215 - accuracy: 0.9943 - val_loss: 0.7394 - val_accuracy: 0.0642\n",
      "Epoch 393/400\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0200 - accuracy: 0.9937\n",
      "Epoch 393: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0201 - accuracy: 0.9935 - val_loss: 0.7467 - val_accuracy: 0.0664\n",
      "Epoch 394/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9955\n",
      "Epoch 394: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0188 - accuracy: 0.9955 - val_loss: 0.7421 - val_accuracy: 0.0642\n",
      "Epoch 395/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0176 - accuracy: 0.9968\n",
      "Epoch 395: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0176 - accuracy: 0.9968 - val_loss: 0.7490 - val_accuracy: 0.0685\n",
      "Epoch 396/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9972\n",
      "Epoch 396: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0167 - accuracy: 0.9972 - val_loss: 0.7478 - val_accuracy: 0.0707\n",
      "Epoch 397/400\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0179 - accuracy: 0.9951\n",
      "Epoch 397: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0179 - accuracy: 0.9951 - val_loss: 0.7565 - val_accuracy: 0.0728\n",
      "Epoch 398/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0170 - accuracy: 0.9968\n",
      "Epoch 398: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.0170 - accuracy: 0.9968 - val_loss: 0.7539 - val_accuracy: 0.0664\n",
      "Epoch 399/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0175 - accuracy: 0.9963\n",
      "Epoch 399: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 2s 19ms/step - loss: 0.0175 - accuracy: 0.9964 - val_loss: 0.7565 - val_accuracy: 0.0600\n",
      "Epoch 400/400\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0175 - accuracy: 0.9951\n",
      "Epoch 400: val_accuracy did not improve from 0.07923\n",
      "78/78 [==============================] - 1s 19ms/step - loss: 0.0175 - accuracy: 0.9951 - val_loss: 0.7628 - val_accuracy: 0.0728\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "data = tf.stack(blue_balls)\n",
    "# 定义超参数\n",
    "n_steps = 150  # 时间步数，即每个样本包含的历史时间步数\n",
    "T = len(data)\n",
    "features = []\n",
    "for i in range(T-n_steps):\n",
    "    features.append(data[i: n_steps + i,:])\n",
    "labels = data[n_steps:,:]\n",
    "features = tf.stack(features) \n",
    "\n",
    "# 将数据集划分为训练集和测试集\n",
    "train_size = int(len(data) * 0.8)\n",
    "train_X, train_y = features[:train_size], labels[:train_size]\n",
    "test_X, test_y = features[train_size:],labels[train_size:]\n",
    "\n",
    "# 创建tf.data.Dataset对象\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_X, train_y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_X, test_y))\n",
    "\n",
    "# 可选：对数据集进行一些预处理操作，例如乱序、批量化和缓存等\n",
    "train_dataset = train_dataset.shuffle(100).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 定义输入和输出形状\n",
    "input_shape = (150, 16)\n",
    "output_shape = (1, 16)\n",
    "# 创建双层LSTM网络\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=input_shape))  # 第一层LSTM\n",
    "model.add(LSTM(64))  # 第二层LSTM\n",
    "model.add(Dense(output_shape[-1], activation='softmax'))  # 输出层\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# 打印模型结构\n",
    "\n",
    "\n",
    "EPOCHS = 400\n",
    "NetNAME = 'B_LSTM2'\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "NAME = timestamp + NetNAME +\"_batchsize\";print(NAME)\n",
    "logdir = \"./logs/\" + NAME\n",
    "modeldir = \"./model/\"+NAME+\".h5\"\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch % 100 == 0 and epoch != 0:\n",
    "        lr = lr / 2\n",
    "    return lr\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=modeldir, monitor='val_accuracy', verbose=1, save_best_only=True, mode = 'max')\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(\n",
    "        x=train_dataset, \n",
    "        epochs=EPOCHS,     \n",
    "        validation_data=test_dataset,    \n",
    "        callbacks=[tensorboard_callback,checkpoint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66cd905-0c1a-4f8f-9066-ca32f105baf1",
   "metadata": {},
   "source": [
    "## 推理验证程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2210e3d9-2d82-4df9-af64-b763dd0cf523",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "红球开奖结果： [0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "蓝球开奖结果： [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[[0.9962148  0.99826235 0.99778795 0.9908155  0.99883455 0.9948278\n",
      "  0.9923968  0.99721897 0.9941993  0.99848217 0.99918145 0.9965025\n",
      "  0.99025273 0.9995783  0.99885345 0.9994025  0.9973465  0.97343314\n",
      "  0.9976089  0.9992656  0.99827456 0.99794513 0.99430037 0.9979247\n",
      "  0.9962347  0.99075454 0.9980453  0.9988097  0.99930334 0.988509\n",
      "  0.9925971  0.9972344  0.9991424 ]]\n",
      "预测结果是红球编号[14 16 29 20 11 33],蓝球编号为9    ---->     很遗憾，你没有中奖\n",
      "预测结果是红球编号[14 16 29 20 11 33],蓝球编号为16    ---->     很遗憾，你没有中奖\n",
      "预测结果是红球编号[14 16 29 20 11 33],蓝球编号为15    ---->     很遗憾，你没有中奖\n",
      "预测结果是红球编号[14 16 29 20 11 33],蓝球编号为3    ---->     很遗憾，你没有中奖\n",
      "预测结果是红球编号[14 16 29 20 11 33],蓝球编号为4    ---->     恭喜你，你中六等奖了！！5块钱\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def CalculateTheAwards(R,B,Rlabel,Blabel):\n",
    "    R = R.numpy()\n",
    "    B = B.numpy()\n",
    "    Rlabel = Rlabel.numpy()\n",
    "    Blabel = Blabel.numpy()\n",
    "    Rcount = 0\n",
    "    Bcount = Blabel[B]\n",
    "    for r in R:\n",
    "        Rcount = Rcount + Rlabel[r]\n",
    "    if Bcount+Rcount ==7:\n",
    "        print('恭喜你，你中一等奖了！！百万大奖')\n",
    "    elif Rcount ==6 and Bcount ==0:\n",
    "        print('恭喜你，你中二等奖了！！十万大奖')\n",
    "    elif Rcount ==5 and Bcount ==1:\n",
    "        print('恭喜你，你中三等奖了！！300块')\n",
    "    elif Bcount+Rcount ==5:\n",
    "        print('恭喜你，你中四等奖了！！200块')\n",
    "    elif Bcount+Rcount ==4:\n",
    "        print('恭喜你，你中五等奖了！！10块钱')\n",
    "    elif Bcount ==1 :\n",
    "        print('恭喜你，你中六等奖了！！5块钱')\n",
    "    else:\n",
    "        print('很遗憾，你没有中奖')\n",
    "# 加载模型\n",
    "Rmodel = load_model('model/20231204-174742LSTM2_lr100_batchsize.h5')   #20231204-173622LSTM2_batchsize.h5\n",
    "Bmodel = load_model('model/20231204-192802B_LSTM2_batchsize.h5')\n",
    "\n",
    "i = 11\n",
    "# 加载新的数据进行推理\n",
    "R = tf.expand_dims(tf.stack(red_balls[-150-i:-i]), axis=0)\n",
    "B = tf.expand_dims(tf.stack(blue_balls[-150-i:-i]), axis=0)\n",
    "Rlabel = red_balls[-i]\n",
    "Blabel = blue_balls[-i]\n",
    "print('红球开奖结果：',Rlabel.numpy().astype(int))\n",
    "print('蓝球开奖结果：',Blabel.numpy().astype(int))\n",
    "# 对新数据进行预测\n",
    "Rpredictions = Rmodel.predict(R,verbose=0)\n",
    "Bpredictions = Bmodel.predict(B,verbose=0)\n",
    "# # 打印预测结果\n",
    "print(Rpredictions)\n",
    "# print(Bpredictions)\n",
    "\n",
    "# 选择出最大的5个元素及其索引\n",
    "_, Rindices = tf.math.top_k(tf.constant(Rpredictions[0]), k=6)\n",
    "_, Bindices = tf.math.top_k(tf.constant(Bpredictions[0]), k=5)\n",
    "print(f'预测结果是红球编号{(Rindices+1).numpy()},蓝球编号为{(Bindices[0]+1).numpy()}',end = '    ---->     ')\n",
    "CalculateTheAwards(Rindices,Bindices[0],Rlabel,Blabel)\n",
    "print(f'预测结果是红球编号{(Rindices+1).numpy()},蓝球编号为{(Bindices[1]+1).numpy()}',end = '    ---->     ')\n",
    "CalculateTheAwards(Rindices,Bindices[1],Rlabel,Blabel)\n",
    "print(f'预测结果是红球编号{(Rindices+1).numpy()},蓝球编号为{(Bindices[2]+1).numpy()}',end = '    ---->     ')\n",
    "CalculateTheAwards(Rindices,Bindices[2],Rlabel,Blabel)\n",
    "print(f'预测结果是红球编号{(Rindices+1).numpy()},蓝球编号为{(Bindices[3]+1).numpy()}',end = '    ---->     ')\n",
    "CalculateTheAwards(Rindices,Bindices[3],Rlabel,Blabel)\n",
    "print(f'预测结果是红球编号{(Rindices+1).numpy()},蓝球编号为{(Bindices[4]+1).numpy()}',end = '    ---->     ')\n",
    "CalculateTheAwards(Rindices,Bindices[4],Rlabel,Blabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad94f5-338f-4d15-a2eb-ae443d01009a",
   "metadata": {},
   "source": [
    "# 双色球应用程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c1c3752b-25e0-4641-829f-99d27b522ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天是: 2023-12-04   推理下一次双色球的号码是：\n",
      "1：红球编号[14 16 29 20 11 33],蓝球编号为1\n",
      "2：红球编号[14 16 29 20 11 33],蓝球编号为10\n",
      "3：红球编号[14 16 29 20 11 33],蓝球编号为12\n",
      "4：红球编号[14 16 29 20 11 33],蓝球编号为7\n",
      "5：红球编号[14 16 29 20 11 33],蓝球编号为13\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime,date\n",
    "from tensorflow.keras.models import load_model\n",
    "# 加载模型\n",
    "Rmodel = load_model('model/20231204-174742LSTM2_lr100_batchsize.h5')   #20231204-173622LSTM2_batchsize.h5\n",
    "Bmodel = load_model('model/20231204-192802B_LSTM2_batchsize.h5')\n",
    "\n",
    "url = 'https://jc.zhcw.com/port/client_json.php'\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0\",\n",
    "    \"Referer\": \"https://www.zhcw.com/\"\n",
    "}\n",
    "cookies = {\n",
    "    \"Hm_lvt_692bd5f9c07d3ebd0063062fb0d7622f\": \"1701622572\",\n",
    "    \"_gid\": \"GA1.2.1218667687.1701622572\",\n",
    "    \"Hm_lvt_12e4883fd1649d006e3ae22a39f97330\": \"1701622573\",\n",
    "    \"PHPSESSID\": \"6k70gq2h44nksmou3n8374jq13\",\n",
    "    \"_ga_9FDP3NWFMS\": \"GS1.1.1701622572.1.1.1701623257.0.0.0\",\n",
    "    \"_ga\": \"GA1.2.1720843243.1701622572\",\n",
    "    \"Hm_lpvt_12e4883fd1649d006e3ae22a39f97330\": \"1701623257\",\n",
    "    \"Hm_lpvt_692bd5f9c07d3ebd0063062fb0d7622f\": \"1701623258\"\n",
    "}\n",
    "\n",
    "history = []\n",
    "for i in range(1,6):\n",
    "    params = {\n",
    "            'callback':'jQuery112208474410773064831_1701622567918',\n",
    "            'transactionType':10001001,\n",
    "            'lotteryId':1,\n",
    "            'issueCount':150,\n",
    "            'startIssue':'',\n",
    "            'endIssue':'',\n",
    "            'startDate':'',\n",
    "            'endDate':'',\n",
    "            'type':0,\n",
    "            'pageNum':i,'pageSize':30,\n",
    "            \"tt\": random.random(),\n",
    "            \"_\": str(int(time.time() * 1000))\n",
    "        }\n",
    "    # 发送HTTP GET请求\n",
    "    response = requests.get(url,headers=headers,cookies=cookies,params=params)\n",
    "\n",
    "    # 提取JSONP响应中的JSON数据\n",
    "    json_data = response.text.split('(')[1].split(')')[0]\n",
    "    # 解析JSON数据\n",
    "    data = json.loads(json_data)\n",
    "    # 提取双色球号码信息\n",
    "    for entry in data['data']:\n",
    "        issue = int(entry['issue'])\n",
    "        front_winning_num = entry['frontWinningNum']\n",
    "        back_winning_num = entry['backWinningNum']\n",
    "        history.append([issue,front_winning_num,back_winning_num])\n",
    "# 按照期号进行排序\n",
    "history = sorted(history, key=lambda x: x[0])\n",
    "#print(history)        \n",
    "\n",
    "red_history = []\n",
    "blue_history = []\n",
    "# 构造模型的输入数据\n",
    "for h in history:\n",
    "        red = np.array([int(i)-1 for i in h[1].split(' ')])\n",
    "        # 编码\n",
    "        red = tf.one_hot(red, depth=33)\n",
    "        red = tf.reduce_sum(red, axis=0)\n",
    "        red_history.append(red)\n",
    "        blue = np.array([int(h[2])-1])\n",
    "        blue = tf.one_hot(blue, depth=16)\n",
    "        blue = tf.reduce_sum(blue, axis=0)\n",
    "        blue_history.append(blue)        \n",
    "#print(blue_balls)       \n",
    "\n",
    "\n",
    "\n",
    "# # 加载新的数据进行推理\n",
    "R = tf.expand_dims(tf.stack(red_history), axis=0)\n",
    "B = tf.expand_dims(tf.stack(blue_history), axis=0)\n",
    "\n",
    "\n",
    "# 对新数据进行预测\n",
    "Rpredictions = Rmodel.predict(R,verbose=0)\n",
    "Bpredictions = Bmodel.predict(B,verbose=0)\n",
    "# # 打印预测结果\n",
    "#print(Rpredictions)\n",
    "# print(Bpredictions)\n",
    "\n",
    "# # 选择出最大的5个元素及其索引\n",
    "_, Rindices = tf.math.top_k(tf.constant(Rpredictions[0]), k=6)\n",
    "_, Bindices = tf.math.top_k(tf.constant(Bpredictions[0]), k=5)\n",
    "print('今天是:', date.today(),'  推理下一次双色球的号码是：')\n",
    "print(f'1：红球编号{(Rindices+1).numpy()},蓝球编号为{(Bindices[0]+1).numpy()}')\n",
    "print(f'2：红球编号{(Rindices+1).numpy()},蓝球编号为{(Bindices[1]+1).numpy()}')\n",
    "print(f'3：红球编号{(Rindices+1).numpy()},蓝球编号为{(Bindices[2]+1).numpy()}')\n",
    "print(f'4：红球编号{(Rindices+1).numpy()},蓝球编号为{(Bindices[3]+1).numpy()}')\n",
    "print(f'5：红球编号{(Rindices+1).numpy()},蓝球编号为{(Bindices[4]+1).numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c4de6e-1283-439a-872b-7bc0eb508a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
